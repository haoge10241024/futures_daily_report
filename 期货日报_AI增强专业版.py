import os
import akshare as ak
import pandas as pd
from datetime import datetime, timedelta
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.oxml.ns import qn
from docx.enum.text import WD_ALIGN_PARAGRAPH
import matplotlib.pyplot as plt
from matplotlib import rcParams
import mplfinance as mpf
import streamlit as st
import requests
import json
import time
from bs4 import BeautifulSoup
import feedparser
import re

# é…ç½®å­—ä½“ï¼ˆå®Œå…¨é¿å…Linuxå­—ä½“é”™è¯¯ï¼‰
import warnings
import logging
import platform

# å½»åº•å¿½ç•¥æ‰€æœ‰matplotlibå­—ä½“è­¦å‘Š
warnings.filterwarnings('ignore')
logging.getLogger('matplotlib').setLevel(logging.CRITICAL)
logging.getLogger('matplotlib.font_manager').setLevel(logging.CRITICAL)

# å¼ºåˆ¶è®¾ç½®matplotlibä½¿ç”¨é»˜è®¤å­—ä½“ï¼Œä¸ä½¿ç”¨ä¸­æ–‡å­—ä½“
import matplotlib
matplotlib.rcParams['font.family'] = 'sans-serif'
matplotlib.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Arial']
matplotlib.rcParams['axes.unicode_minus'] = False

# Windowsæœ¬åœ°å¼€å‘æ—¶ä½¿ç”¨ä¸­æ–‡å­—ä½“ï¼ˆä»…ç”¨äºå…¼å®¹ï¼‰
if platform.system() == 'Windows':
    try:
        matplotlib.rcParams['font.sans-serif'] = ['Microsoft YaHei', 'Arial']
    except:
        pass

# ============ APIé…ç½® ============
# ç›´æ¥é…ç½®APIå¯†é’¥ï¼ˆå†…ç½®å¯†é’¥ï¼Œæ— éœ€ç”¨æˆ·é…ç½®ï¼‰
DEFAULT_DEEPSEEK_API_KEY = "sk-293dec7fabb54606b4f8d4f606da3383"
DEFAULT_SERPER_API_KEY = "e62b7b8b688821905ad0d6c360e44813175792fc"
DEEPSEEK_API_URL = "https://api.deepseek.com/v1/chat/completions"

# å…¨å±€APIå¯†é’¥ï¼ˆç›´æ¥ä½¿ç”¨ï¼Œä¸éœ€è¦ç”¨æˆ·é…ç½®ï¼‰
DEEPSEEK_API_KEY = DEFAULT_DEEPSEEK_API_KEY
SERPER_API_KEY = DEFAULT_SERPER_API_KEY


class EnhancedNewsSearcher:
    """å¢å¼ºçš„æœŸè´§æ–°é—»æœç´¢å™¨"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        
        # RSSè®¢é˜…æº
        self.rss_feeds = {
            'ä¸œæ–¹è´¢å¯ŒæœŸè´§': 'http://feed.eastmoney.com/rss/futures.xml',
            'é‡‘èç•ŒæœŸè´§': 'http://rss.jrj.com.cn/rss/futures_index.xml',
            'æ–°æµªè´¢ç»': 'http://rss.sina.com.cn/finance/futures.xml'
        }
    
    def search_with_serper_api(self, commodity: str, days_back: int = 3, api_key: str = None, target_dates: list = None):
        """ä½¿ç”¨Serper APIè¿›è¡Œæœç´¢ï¼ˆæ”¯æŒæŒ‡å®šç›®æ ‡æ—¥æœŸåˆ—è¡¨ï¼‰"""
        if not api_key:
            return []
        
        try:
            # target_dateså¯ä»¥æ˜¯å•ä¸ªæ—¥æœŸå­—ç¬¦ä¸²æˆ–æ—¥æœŸåˆ—è¡¨
            if target_dates is None:
                dates_to_search = [datetime.now().strftime('%Y-%m-%d')]
            elif isinstance(target_dates, str):
                dates_to_search = [target_dates]
            elif isinstance(target_dates, list):
                dates_to_search = target_dates
            else:
                dates_to_search = [datetime.now().strftime('%Y-%m-%d')]
            
            all_news = []
            for target_date in dates_to_search:
                base_date = datetime.strptime(target_date, '%Y-%m-%d')
                
                # æœç´¢ç›®æ ‡æ—¥æœŸçš„æ–°é—»
                search_query = f'{commodity}æœŸè´§ OR {commodity}ä»·æ ¼ OR {commodity}å¸‚åœº {base_date.strftime("%Yå¹´%mæœˆ%dæ—¥")}'
                
                url = "https://google.serper.dev/search"
                payload = json.dumps({
                    "q": search_query,
                    "num": 50,  # å¢åŠ è¿”å›æ•°é‡
                    "tbs": "qdr:d",  # åªæœç´¢å½“å¤©
                    "gl": "cn",
                    "hl": "zh-cn"
                })
                
                headers = {
                    'X-API-KEY': api_key,
                    'Content-Type': 'application/json'
                }
                
                response = requests.post(url, headers=headers, data=payload, timeout=30)
                
                if response.status_code == 200:
                    results = response.json()
                    
                    for item in results.get('organic', []):
                        if self._is_relevant_financial_news(item.get('title', ''), item.get('snippet', ''), commodity):
                            news_item = {
                                'title': item.get('title', ''),
                                'content': item.get('snippet', ''),
                                'url': item.get('link', ''),
                                'source': item.get('displayedLink', 'è´¢ç»èµ„è®¯'),
                                'date': target_date,
                                'relevance': self._calculate_relevance(item.get('title', '') + item.get('snippet', ''), commodity)
                            }
                            all_news.append(news_item)
                
                time.sleep(0.5)  # é¿å…APIé™åˆ¶
            
            # è¿”å›æ‰€æœ‰æœç´¢åˆ°çš„æ–°é—»ï¼ˆä¸é™åˆ¶æ•°é‡ï¼‰
            return all_news
                
        except Exception as e:
            print(f"  âŒ Serperæœç´¢å‡ºé”™: {e}")
            return []
    
    def scrape_eastmoney_news(self, commodity: str, days_back: int = 3):
        """çˆ¬å–ä¸œæ–¹è´¢å¯ŒæœŸè´§æ–°é—»"""
        try:
            search_url = f"http://so.eastmoney.com/news/s?keyword={commodity}æœŸè´§"
            
            response = self.session.get(search_url, timeout=15)
            response.encoding = 'utf-8'
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                news_list = []
                
                news_items = soup.find_all(['div', 'li'], class_=['news-item', 'result-item', 'item'])
                
                for item in news_items[:20]:
                    try:
                        title_elem = item.find(['a', 'h3'], href=True)
                        if title_elem:
                            title = title_elem.get_text(strip=True)
                            url = title_elem.get('href', '')
                            
                            content_elem = item.find(['p', 'div'], class_=['summary', 'content', 'desc'])
                            content = content_elem.get_text(strip=True) if content_elem else ''
                            
                            date_elem = item.find(['span', 'div'], class_=['time', 'date', 'publish-time'])
                            date_str = date_elem.get_text(strip=True) if date_elem else ''
                            
                            if title and self._is_relevant_financial_news(title, content, commodity):
                                news_item = {
                                    'title': title,
                                    'content': content,
                                    'url': url if url.startswith('http') else f"http://futures.eastmoney.com{url}",
                                    'source': 'ä¸œæ–¹è´¢å¯Œ',
                                    'date': self._parse_date(date_str),
                                    'relevance': self._calculate_relevance(title + content, commodity)
                                }
                                news_list.append(news_item)
                    except Exception:
                        continue
                
                return sorted(news_list, key=lambda x: x['relevance'], reverse=True)[:10]
            else:
                return []
                
        except Exception as e:
            print(f"  âŒ ä¸œæ–¹è´¢å¯Œçˆ¬å–å‡ºé”™: {e}")
            return []
    
    def scrape_jrj_news(self, commodity: str, days_back: int = 3):
        """çˆ¬å–é‡‘èç•ŒæœŸè´§æ–°é—»"""
        try:
            search_url = f"http://search.jrj.com.cn/?q={commodity}æœŸè´§&t=news"
            
            response = self.session.get(search_url, timeout=15)
            response.encoding = 'gbk'
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                news_list = []
                
                news_items = soup.find_all(['div', 'li'], class_=['search-result', 'news-item'])
                
                for item in news_items[:15]:
                    try:
                        title_elem = item.find('a')
                        if title_elem:
                            title = title_elem.get_text(strip=True)
                            url = title_elem.get('href', '')
                            
                            content_elem = item.find(['p', 'span'], class_=['summary', 'desc'])
                            content = content_elem.get_text(strip=True) if content_elem else ''
                            
                            if title and self._is_relevant_financial_news(title, content, commodity):
                                news_item = {
                                    'title': title,
                                    'content': content,
                                    'url': url,
                                    'source': 'é‡‘èç•Œ',
                                    'date': datetime.now().strftime('%Y-%m-%d'),
                                    'relevance': self._calculate_relevance(title + content, commodity)
                                }
                                news_list.append(news_item)
                    except Exception:
                        continue
                
                return sorted(news_list, key=lambda x: x['relevance'], reverse=True)[:10]
            else:
                return []
                
        except Exception as e:
            print(f"  âŒ é‡‘èç•Œçˆ¬å–å‡ºé”™: {e}")
            return []
    
    def get_rss_news(self, commodity: str, days_back: int = 3):
        """ä»RSSè®¢é˜…æºè·å–æ–°é—»"""
        try:
            all_news = []
            cutoff_date = datetime.now() - timedelta(days=days_back)
            
            for feed_name, feed_url in self.rss_feeds.items():
                try:
                    feed = feedparser.parse(feed_url)
                    
                    for entry in feed.entries[:20]:
                        title = entry.get('title', '')
                        summary = entry.get('summary', entry.get('description', ''))
                        link = entry.get('link', '')
                        
                        pub_date = entry.get('published_parsed')
                        if pub_date:
                            pub_datetime = datetime(*pub_date[:6])
                            if pub_datetime < cutoff_date:
                                continue
                        
                        if self._is_relevant_financial_news(title, summary, commodity):
                            news_item = {
                                'title': title,
                                'content': summary,
                                'url': link,
                                'source': feed_name.replace('_RSS', ''),
                                'date': pub_datetime.strftime('%Y-%m-%d') if pub_date else datetime.now().strftime('%Y-%m-%d'),
                                'relevance': self._calculate_relevance(title + summary, commodity)
                            }
                            all_news.append(news_item)
                    
                    time.sleep(1)
                    
                except Exception as e:
                    continue
            
            return sorted(all_news, key=lambda x: x['relevance'], reverse=True)[:10]
            
        except Exception as e:
            return []
    
    def _is_relevant_financial_news(self, title: str, content: str, commodity: str) -> bool:
        """åˆ¤æ–­æ–°é—»æ˜¯å¦ä¸æœŸè´§å“ç§ç›¸å…³"""
        text = (title + ' ' + content).lower()
        
        futures_keywords = ['æœŸè´§', 'ä»·æ ¼', 'å¸‚åœº', 'åˆçº¦', 'äº¤æ˜“', 'æ¶¨è·Œ', 'è¡Œæƒ…', 'åˆ†æ', 'é¢„æµ‹']
        commodity_keywords = [commodity.lower(), f'{commodity}ä»·æ ¼', f'{commodity}å¸‚åœº']
        
        has_commodity = any(keyword in text for keyword in commodity_keywords)
        has_futures = any(keyword in text for keyword in futures_keywords)
        
        return has_commodity and has_futures
    
    def _calculate_relevance(self, text: str, commodity: str) -> float:
        """è®¡ç®—æ–°é—»ä¸å•†å“çš„ç›¸å…³æ€§å¾—åˆ†"""
        text = text.lower()
        score = 0.0
        
        if commodity.lower() in text:
            score += 5.0
        
        futures_words = ['æœŸè´§', 'ä»·æ ¼', 'æ¶¨è·Œ', 'è¡Œæƒ…', 'åˆçº¦', 'äº¤æ˜“']
        for word in futures_words:
            if word in text:
                score += 1.0
        
        timely_words = ['ä»Šæ—¥', 'æ˜¨æ—¥', 'æœ€æ–°', 'æœ€è¿‘', 'ä»Šå¤©']
        for word in timely_words:
            if word in text:
                score += 0.5
        
        return min(score, 10.0)
    
    def _parse_date(self, date_str: str) -> str:
        """è§£ææ—¥æœŸå­—ç¬¦ä¸²"""
        if not date_str:
            return datetime.now().strftime('%Y-%m-%d')
        
        try:
            date_str = re.sub(r'[å¹´æœˆ]', '-', date_str)
            date_str = re.sub(r'æ—¥', '', date_str)
            
            if 'ä»Šå¤©' in date_str or 'ä»Šæ—¥' in date_str:
                return datetime.now().strftime('%Y-%m-%d')
            elif 'æ˜¨å¤©' in date_str or 'æ˜¨æ—¥' in date_str:
                return (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')
            else:
                return datetime.now().strftime('%Y-%m-%d')
        except:
            return datetime.now().strftime('%Y-%m-%d')
    
    def search_professional_data(self, commodity: str, serper_key: str, target_date: str = None):
        """æœç´¢ä¸“ä¸šæœŸè´§åˆ†ææ•°æ®ï¼ˆ8å¤§ç»´åº¦ï¼‰"""
        try:
            if not target_date:
                target_date = datetime.now().strftime('%Y-%m-%d')
            
            # 8å¤§ä¸“ä¸šåˆ†æç»´åº¦çš„æœç´¢å…³é”®è¯
            professional_keywords = {
                # 1. åº“å­˜ä»“å•æ•°æ®
                'åº“å­˜ä»“å•': [
                    f"{commodity} ä»“å• {target_date}",
                    f"{commodity} åº“å­˜ {target_date}",
                    f"{commodity} äº¤æ˜“æ‰€ä»“å•"
                ],
                # 2. åŸºå·®æ•°æ®
                'åŸºå·®åˆ†æ': [
                    f"{commodity} åŸºå·® {target_date}",
                    f"{commodity} æœŸç°ä»·å·®",
                    f"{commodity} ç°è´§ä»·æ ¼ æœŸè´§ä»·æ ¼"
                ],
                # 3. æœŸé™ç»“æ„
                'æœŸé™ç»“æ„': [
                    f"{commodity} æœˆå·® {target_date}",
                    f"{commodity} è¿œè¿‘æœˆä»·å·®",
                    f"{commodity} è·¨æœŸä»·å·®"
                ],
                # 4. æŒä»“å¸­ä½
                'æŒä»“å¸­ä½': [
                    f"{commodity} æŒä»“å¸­ä½ {target_date}",
                    f"{commodity} ä¸»åŠ›æŒä»“",
                    f"{commodity} å¤šç©ºæŒä»“"
                ],
                # 5. ä¾›éœ€æ•°æ®
                'ä¾›éœ€åˆ†æ': [
                    f"{commodity} äº§é‡ {target_date}",
                    f"{commodity} æ¶ˆè´¹é‡",
                    f"{commodity} ä¾›éœ€å¹³è¡¡è¡¨"
                ],
                # 6. äº§ä¸šé“¾æ•°æ®
                'äº§ä¸šé“¾': [
                    f"{commodity} äº§ä¸šé“¾ ä»·æ ¼",
                    f"{commodity} ä¸Šä¸‹æ¸¸",
                    f"{commodity} ç”Ÿäº§åˆ©æ¶¦"
                ],
                # 7. è¿›å‡ºå£æ•°æ®
                'è¿›å‡ºå£': [
                    f"{commodity} è¿›å£é‡ {target_date}",
                    f"{commodity} å‡ºå£é‡",
                    f"{commodity} æµ·å…³æ•°æ®"
                ],
                # 8. å®è§‚æ”¿ç­–
                'å®è§‚æ”¿ç­–': [
                    f"{commodity} æ”¿ç­– {target_date}",
                    f"{commodity} è¡Œä¸šæ”¿ç­–",
                    f"{commodity} å›½å®¶æ”¿ç­–"
                ]
            }
            
            professional_data = {}
            url = "https://google.serper.dev/search"
            
            for category, keywords in professional_keywords.items():
                category_data = []
                
                for keyword in keywords[:2]:  # æ¯ä¸ªç»´åº¦æœç´¢å‰2ä¸ªå…³é”®è¯
                    try:
                        payload = json.dumps({
                            "q": keyword,
                            "num": 5,
                            "gl": "cn",
                            "hl": "zh-cn"
                        })
                        
                        headers = {
                            'X-API-KEY': serper_key,
                            'Content-Type': 'application/json'
                        }
                        
                        response = requests.post(url, headers=headers, data=payload, timeout=30)
                        
                        if response.status_code == 200:
                            results = response.json()
                            for item in results.get('organic', [])[:2]:  # æ¯ä¸ªå…³é”®è¯å–å‰2æ¡
                                category_data.append({
                                    'title': item.get('title', ''),
                                    'content': item.get('snippet', ''),
                                    'url': item.get('link', ''),
                                    'source': item.get('displayedLink', 'æœªçŸ¥'),
                                    'date': target_date,
                                    'category': category
                                })
                        
                        time.sleep(0.5)  # é¿å…APIé¢‘ç‡é™åˆ¶
                    except Exception as e:
                        print(f"  âš ï¸ æœç´¢{category}æ•°æ®å‡ºé”™: {e}")
                        continue
                
                if category_data:
                    professional_data[category] = category_data
            
            return professional_data
        except Exception as e:
            print(f"  âš ï¸ ä¸“ä¸šæ•°æ®æœç´¢å‡ºé”™: {e}")
            return {}
    
    def comprehensive_search(self, commodity: str, days_back: int = 3, serper_key: str = None, target_dates: list = None):
        """ç»¼åˆæœç´¢ï¼šç»“åˆå¤šç§æ•°æ®æºï¼ˆæ”¯æŒæ—¥æœŸåˆ—è¡¨ï¼‰"""
        all_news = []
        
        # 1. Serper APIæœç´¢å•†å“æ–°é—»ï¼ˆä¼ é€’target_datesæ—¥æœŸåˆ—è¡¨ï¼‰
        if serper_key:
            serper_news = self.search_with_serper_api(commodity, days_back, serper_key, target_dates)
            all_news.extend(serper_news)
        
        # 2. ç½‘é¡µçˆ¬è™«
        eastmoney_news = self.scrape_eastmoney_news(commodity, days_back)
        all_news.extend(eastmoney_news)
        
        jrj_news = self.scrape_jrj_news(commodity, days_back)
        all_news.extend(jrj_news)
        
        # 3. RSSè®¢é˜…
        rss_news = self.get_rss_news(commodity, days_back)
        all_news.extend(rss_news)
        
        # å»é‡å’Œæ’åº
        seen_titles = set()
        unique_news = []
        
        for news in all_news:
            title = news['title']
            if title not in seen_titles:
                seen_titles.add(title)
                unique_news.append(news)
        
        unique_news.sort(key=lambda x: x['relevance'], reverse=True)
        
        return unique_news[:25]


# ============ æŠ€æœ¯æŒ‡æ ‡è®¡ç®—å‡½æ•° ============

def calculate_technical_indicators(market_data_df):
    """è®¡ç®—æŠ€æœ¯æŒ‡æ ‡ï¼ˆMAã€MACDã€RSIç­‰ï¼‰"""
    try:
        if market_data_df.empty or len(market_data_df) < 20:
            return {}
        
        close_prices = market_data_df['close']
        
        # è®¡ç®—å‡çº¿
        ma5 = close_prices.rolling(window=5).mean().iloc[-1] if len(close_prices) >= 5 else None
        ma10 = close_prices.rolling(window=10).mean().iloc[-1] if len(close_prices) >= 10 else None
        ma20 = close_prices.rolling(window=20).mean().iloc[-1] if len(close_prices) >= 20 else None
        
        # è®¡ç®—MACD
        exp1 = close_prices.ewm(span=12, adjust=False).mean()
        exp2 = close_prices.ewm(span=26, adjust=False).mean()
        macd = exp1 - exp2
        signal = macd.ewm(span=9, adjust=False).mean()
        macd_hist = macd - signal
        
        # è®¡ç®—RSI
        delta = close_prices.diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
        rs = gain / loss
        rsi = 100 - (100 / (1 + rs))
        
        # è®¡ç®—å¸ƒæ—å¸¦
        ma20_series = close_prices.rolling(window=20).mean()
        std20 = close_prices.rolling(window=20).std()
        upper_band = ma20_series + (std20 * 2)
        lower_band = ma20_series - (std20 * 2)
        
        current_price = close_prices.iloc[-1]
        
        return {
            'ma5': round(ma5, 2) if ma5 else None,
            'ma10': round(ma10, 2) if ma10 else None,
            'ma20': round(ma20, 2) if ma20 else None,
            'macd': round(macd.iloc[-1], 2) if len(macd) > 0 else None,
            'macd_signal': round(signal.iloc[-1], 2) if len(signal) > 0 else None,
            'macd_hist': round(macd_hist.iloc[-1], 2) if len(macd_hist) > 0 else None,
            'rsi': round(rsi.iloc[-1], 2) if len(rsi) > 0 and not pd.isna(rsi.iloc[-1]) else None,
            'upper_band': round(upper_band.iloc[-1], 2) if len(upper_band) > 0 else None,
            'lower_band': round(lower_band.iloc[-1], 2) if len(lower_band) > 0 else None,
            'current_price': round(current_price, 2),
            'price_position': 'ä¸Šè½¨é™„è¿‘' if current_price > upper_band.iloc[-1] else ('ä¸‹è½¨é™„è¿‘' if current_price < lower_band.iloc[-1] else 'ä¸­è½¨é™„è¿‘') if len(upper_band) > 0 else 'ä¸­è½¨'
        }
    except Exception as e:
        print(f"  âš ï¸ æŠ€æœ¯æŒ‡æ ‡è®¡ç®—å‡ºé”™: {e}")
        return {}


# ============ AIè¾…åŠ©å‡½æ•° ============

def ai_generate_market_description(market_data: dict, commodity_name: str, date_str: str, technical_indicators: dict = None, market_date=None) -> str:
    """ä½¿ç”¨DeepSeek AIç”Ÿæˆè¡Œæƒ…æè¿°ï¼ˆå¼ºåŒ–çœŸå®æ€§çº¦æŸï¼‰
    
    Args:
        market_data: å¸‚åœºæ•°æ®å­—å…¸
        commodity_name: å•†å“åç§°
        date_str: æ—¥æŠ¥ç”Ÿæˆæ—¥æœŸ
        technical_indicators: æŠ€æœ¯æŒ‡æ ‡
        market_date: å®é™…çš„å¸‚åœºå›é¡¾æ—¥æœŸï¼ˆäº¤æ˜“æ—¥ï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨è®¡ç®—å‰ä¸€å¤©
    """
    print(f"[INFO] ========== AIç”Ÿæˆè¡Œæƒ…æè¿° ==========")
    print(f"[INFO] å“ç§: {commodity_name}, æ—¥æŠ¥æ—¥æœŸ: {date_str}")
    
    # å¦‚æœæä¾›äº†å®é™…çš„å¸‚åœºæ—¥æœŸï¼Œç›´æ¥ä½¿ç”¨ï¼›å¦åˆ™è®¡ç®—å‰ä¸€å¤©
    if market_date is None:
        report_date = datetime.strptime(date_str, '%Y-%m-%d')
        market_date = (report_date - timedelta(days=1)).strftime('%Y-%m-%d')
    elif isinstance(market_date, datetime):
        market_date = market_date.strftime('%Y-%m-%d')
    
    print(f"[INFO] å¸‚åœºå›é¡¾æ—¥æœŸ: {market_date}")
    print(f"[INFO] APIå¯†é’¥é•¿åº¦: {len(DEEPSEEK_API_KEY)}")
    print(f"[INFO] API URL: {DEEPSEEK_API_URL}")
    try:
        prompt = f"""
ä½ æ˜¯ä¸“ä¸šæœŸè´§åˆ†æå¸ˆï¼Œæ­£åœ¨ä¸º{date_str}æ’°å†™{commodity_name}æœŸè´§çš„å¸‚åœºèµ°åŠ¿æè¿°ã€‚

ã€é‡è¦çº¦æŸã€‘
1. âš ï¸ å¿…é¡»ä¸¥æ ¼åŸºäºä¸‹æ–¹æä¾›çš„çœŸå®è¡Œæƒ…æ•°æ®ï¼Œä¸¥ç¦ç¼–é€ ä»»ä½•ä»·æ ¼æˆ–æ¶¨è·Œå¹…
2. âš ï¸ å¦‚æœæŸé¡¹æ•°æ®æ˜¾ç¤ºä¸º"N/A"ï¼Œä¸è¦æ¨æµ‹æˆ–ç¼–é€ ï¼Œç›´æ¥è¯´æ˜æ•°æ®ç¼ºå¤±
3. âš ï¸ å¼•ç”¨çš„æ‰€æœ‰æ•°å­—å¿…é¡»ä¸æä¾›çš„æ•°æ®å®Œå…¨ä¸€è‡´ï¼Œä¸è¦å››èˆäº”å…¥æˆ–ä¿®æ”¹
4. âš ï¸ åˆ†ææ—¥æœŸä¸º{date_str}ï¼Œä¸è¦ä½¿ç”¨å…¶ä»–æ—¥æœŸ

ã€{commodity_name}æœŸè´§çœŸå®è¡Œæƒ…æ•°æ® - {date_str}ã€‘

æ—¥ç›˜æ•°æ®ï¼š
- å¼€ç›˜ä»·ï¼š{market_data.get('open', 'N/A')}å…ƒ
- æ”¶ç›˜ä»·ï¼š{market_data.get('close', 'N/A')}å…ƒ
- æœ€é«˜ä»·ï¼š{market_data.get('high', 'N/A')}å…ƒ
- æœ€ä½ä»·ï¼š{market_data.get('low', 'N/A')}å…ƒ
- æ¶¨è·Œé¢ï¼š{market_data.get('change', 'N/A')}å…ƒ
- æ¶¨è·Œå¹…ï¼š{market_data.get('change_pct', 'N/A')}%

å¤œç›˜æ•°æ®ï¼š
- å¼€ç›˜ä»·ï¼š{market_data.get('night_open', 'N/A')}å…ƒ
- æ”¶ç›˜ä»·ï¼š{market_data.get('night_close', 'N/A')}å…ƒ
- æ¶¨è·Œé¢ï¼š{market_data.get('night_change', 'N/A')}å…ƒ
- æ¶¨è·Œå¹…ï¼š{market_data.get('night_change_pct', 'N/A')}%

ã€æ’°å†™è¦æ±‚ã€‘
è¯·æ’°å†™ä¸€æ®µ160-200å­—çš„ä¸“ä¸šè¡Œæƒ…èµ°åŠ¿æè¿°ï¼Œå¿…é¡»åŒ…å«ï¼š
1. æ—¥ç›˜èµ°åŠ¿ - å¼€ç›˜â†’ç›˜ä¸­é«˜ä½ç‚¹â†’æ”¶ç›˜ï¼Œæè¿°ä»·æ ¼è¿è¡Œè½¨è¿¹
2. å¤œç›˜èµ°åŠ¿ - å¼€ç›˜â†’æ”¶ç›˜ï¼Œä¸æ—¥ç›˜å¯¹æ¯”åˆ†æ
3. æŠ€æœ¯å½¢æ€ - å¦‚"åå­—æ˜Ÿ"ã€"é•¿é˜³çº¿"ã€"è·³ç©º"ç­‰ï¼ˆåŸºäºçœŸå®æ•°æ®åˆ¤æ–­ï¼‰
4. å…³é”®ç‚¹ä½ - æ”¯æ’‘ä½ã€å‹åŠ›ä½ï¼ˆåŸºäºç»™å®šçš„é«˜ä½ä»·ï¼‰
5. å¸‚åœºæƒ…ç»ª - å¤šç©ºåŠ›é‡å¯¹æ¯”ï¼ˆåŸºäºæ¶¨è·Œæ¨æ–­ï¼‰

ã€æ–‡é£è¦æ±‚ã€‘
- ä½¿ç”¨ä¸“ä¸šæœ¯è¯­ï¼šå¦‚"æ‰¿å‹å›è½"ã€"è·å¾—æ”¯æ’‘"ã€"éœ‡è¡æ•´ç†"ç­‰
- è¯­è¨€ç²¾å‡†ï¼šæ¯ä¸ªä»·æ ¼éƒ½è¦æœ‰æ¥æºä¾æ®
- é€»è¾‘æ¸…æ™°ï¼šæ—¥ç›˜â†’å¤œç›˜â†’å…¨å¤©æ€»ç»“
- å®¢è§‚æè¿°ï¼šä¸æ·»åŠ æœªæä¾›çš„æˆäº¤é‡ã€æŒä»“é‡ç­‰æ•°æ®

âš ï¸ å†æ¬¡å¼ºè°ƒï¼šæ‰€æœ‰æ•°å­—å¿…é¡»æ¥æºäºä¸Šè¿°çœŸå®æ•°æ®ï¼Œä¸è¦ç¼–é€ ï¼

è¯·ç›´æ¥è¾“å‡ºæè¿°æ–‡æœ¬ï¼ˆä¸è¦æ ‡é¢˜ã€ä¸è¦å‰è¨€ï¼‰ï¼š
"""
        
        headers = {
            "Authorization": f"Bearer {DEEPSEEK_API_KEY}",
            "Content-Type": "application/json"
        }
        
        data = {
            "model": "deepseek-chat",
            "messages": [{"role": "user", "content": prompt}],
            "temperature": 0.2,  # é™ä½æ¸©åº¦ï¼Œç¡®ä¿æ•°æ®å‡†ç¡®æ€§
            "max_tokens": 600
        }
        
        response = requests.post(DEEPSEEK_API_URL, headers=headers, json=data, timeout=60)
        
        print(f"[DEBUG] AIç”Ÿæˆè¡Œæƒ…æè¿° - çŠ¶æ€ç : {response.status_code}")
        
        if response.status_code == 200:
            result = response.json()
            content = result['choices'][0]['message']['content'].strip()
            print(f"[DEBUG] AIç”Ÿæˆè¡Œæƒ…æè¿° - è¿”å›å†…å®¹é•¿åº¦: {len(content)}å­—ç¬¦")
            print(f"[DEBUG] AIç”Ÿæˆè¡Œæƒ…æè¿° - å‰100å­—ç¬¦: {content[:100]}")
            return content
        else:
            error_msg = f"AIç”Ÿæˆå¤±è´¥ (çŠ¶æ€ç : {response.status_code})"
            print(f"[ERROR] {error_msg}")
            return error_msg
            
    except Exception as e:
        error_msg = f"AIç”Ÿæˆå‡ºé”™: {str(e)}"
        print(f"[ERROR] {error_msg}")
        return error_msg


def ai_generate_news_summary(commodity_name: str, date_str: str, news_list: list, professional_data: dict = None) -> str:
    """ä½¿ç”¨DeepSeek AIç”Ÿæˆæ–°é—»èµ„è®¯æ‘˜è¦"""
    try:
        # å‡†å¤‡æ‰€æœ‰æ–°é—»æ•°æ®
        all_news = []
        ref_index = 1
        
        # å•†å“æ–°é—»
        for news in news_list[:15]:  # æœ€å¤š15æ¡å•†å“æ–°é—»
            all_news.append({
                'index': ref_index,
                'title': news.get('title', ''),
                'content': news.get('content', ''),
                'source': news.get('source', 'æœªçŸ¥'),
                'date': news.get('date', 'N/A'),
                'url': news.get('url', 'æ— ')
            })
            ref_index += 1
        
        # ä¸“ä¸šç»´åº¦æ•°æ®
        if professional_data:
            for category, data_list in professional_data.items():
                for data in data_list[:2]:  # æ¯ä¸ªç»´åº¦2æ¡
                    all_news.append({
                        'index': ref_index,
                        'title': f"[{category}] {data.get('title', '')}",
                        'content': data.get('content', ''),
                        'source': data.get('source', 'æœªçŸ¥'),
                        'date': date_str,
                        'url': data.get('url', 'æ— ')
                    })
                    ref_index += 1
        
        # æ„å»ºæ–°é—»æ‘˜è¦
        news_summary = ""
        for news in all_news:
            news_summary += f"[{news['index']}] {news['title']}\n"
            if news['content']:
                content = news['content'][:200].replace('...', '').replace('â€¦', '')
                news_summary += f"    {content}\n"
            news_summary += f"    æ¥æºï¼š{news['source']} | æ—¥æœŸï¼š{news['date']}\n\n"
        
        prompt = f"""
ä½ æ˜¯ä¸“ä¸šæœŸè´§åˆ†æå¸ˆï¼Œæ­£åœ¨ä¸º{date_str}çš„{commodity_name}æœŸè´§æ—¥æŠ¥æ•´ç†æ–°é—»èµ„è®¯ã€‚

ã€é‡è¦çº¦æŸã€‘
1. âš ï¸ å¿…é¡»åŸºäºä¸‹æ–¹æä¾›çš„çœŸå®æ–°é—»ï¼Œä¸¥ç¦ç¼–é€ ä»»ä½•ä¿¡æ¯
2. âš ï¸ åªæå–å’Œè¾“å‡ºæ–°é—»çš„ä¸»ä½“å†…å®¹ï¼Œä¸è¦æ·»åŠ è¯„è®ºæˆ–åˆ†æ
3. âš ï¸ æ¯æ¡æ–°é—»ä¿æŒç‹¬ç«‹ï¼Œä¸è¦åˆå¹¶æˆ–é‡å†™
4. âš ï¸ ä½¿ç”¨ä¸Šæ ‡[1][2][3]æ ‡æ³¨æ–°é—»æ¥æº
5. âš ï¸ æ—¥æœŸä¸º{date_str}ï¼Œä¼˜å…ˆé€‰æ‹©å½“å¤©æˆ–ä¸´è¿‘æ—¥æœŸçš„æ–°é—»

ã€æ–°é—»åŸå§‹æ•°æ®ã€‘
{news_summary}

ã€æ•´ç†è¦æ±‚ã€‘
è¯·å°†ä¸Šè¿°**æ‰€æœ‰æ–°é—»**æ•´ç†è¾“å‡ºï¼Œè¦æ±‚ï¼š

1. **è¾“å‡ºæ‰€æœ‰æ–°é—»** - ä¸è¦ç­›é€‰æˆ–åˆ é™¤ä»»ä½•æ–°é—»ï¼Œå…¨éƒ¨è¾“å‡º
2. **æ—¶æ•ˆæ€§æ’åº** - æŒ‰ç…§æ—¥æœŸæ’åºï¼Œ{date_str}çš„æ–°é—»æ’åœ¨å‰é¢
3. **å†…å®¹å®Œæ•´** - ä¿ç•™æ–°é—»ä¸»ä½“å†…å®¹ï¼Œä¸æˆªæ–­
4. **å»é™¤é‡å¤** - ç›¸ä¼¼å†…å®¹åªä¿ç•™ä¸€æ¡
5. **æ ¼å¼ç»Ÿä¸€** - æ¯æ¡æ–°é—»æ ¼å¼ä¸€è‡´

ã€è¾“å‡ºæ ¼å¼ã€‘
åºå·. æ–°é—»ä¸»ä½“å†…å®¹[ä¸Šæ ‡åºå·]

ä¾‹å¦‚ï¼š
1. ä»Šæ—¥ç”ŸçŒªæœŸè´§ä¸»åŠ›åˆçº¦æ”¶äº11325å…ƒï¼Œæ—¥å†…ä¸‹è·Œ2.12%ï¼Œåˆ›è¿‘æœŸæ–°ä½[1]

2. å…¨å›½ç”ŸçŒªå‡ä»·è·Œè‡³11.19å…ƒ/å…¬æ–¤ï¼Œç¯æ¯”ä¸‹è·Œ0.26å…ƒï¼Œæœˆç¯æ¯”ä¸‹è·Œ16.04%[2]

3. äº¤æ˜“æ‰€ä»“å•æ•°æ®æ˜¾ç¤ºï¼Œç”ŸçŒªä»“å•ç¯æ¯”å¢åŠ 3000å¨è‡³15000å¨ï¼Œä¾›åº”å‹åŠ›æŒç»­[3]

...ï¼ˆè¾“å‡ºæ‰€æœ‰æ–°é—»ï¼‰

âš ï¸ é‡è¦æç¤ºï¼š
- ç›´æ¥è¾“å‡ºæ–°é—»å†…å®¹ï¼Œä¸è¦æ·»åŠ "èµ„è®¯æ±‡æ€»"ã€"å¸‚åœºåŠ¨æ€"ç­‰æ ‡é¢˜
- ä¸è¦å¯¹æ–°é—»è¿›è¡Œè¯„è®ºã€åˆ†ææˆ–æ€»ç»“
- ä¸è¦åˆå¹¶å¤šæ¡æ–°é—»
- ä¿æŒæ–°é—»çš„å®¢è§‚æ€§å’Œç‹¬ç«‹æ€§
- æ¯æ¡æ–°é—»æœ«å°¾å¿…é¡»æœ‰ä¸Šæ ‡å¼•ç”¨[æ•°å­—]
- **å¿…é¡»è¾“å‡ºæ‰€æœ‰æä¾›çš„æ–°é—»ï¼Œä¸è¦é—æ¼**

è¯·ç›´æ¥è¾“å‡ºæ•´ç†åçš„æ–°é—»åˆ—è¡¨ï¼š
"""
        
        headers = {
            "Authorization": f"Bearer {DEEPSEEK_API_KEY}",
            "Content-Type": "application/json"
        }
        
        data = {
            "model": "deepseek-chat",
            "messages": [{"role": "user", "content": prompt}],
            "temperature": 0.1,  # æä½æ¸©åº¦ï¼Œç¡®ä¿å¿ å®åŸæ–‡
            "max_tokens": 4000   # æ”¯æŒè¾“å‡ºæ‰€æœ‰æ–°é—»ï¼ˆçº¦20-30æ¡ï¼‰
        }
        
        response = requests.post(DEEPSEEK_API_URL, headers=headers, json=data, timeout=90)
        
        if response.status_code == 200:
            result = response.json()
            return result['choices'][0]['message']['content'].strip()
        else:
            return f"AIç”Ÿæˆå¤±è´¥ (çŠ¶æ€ç : {response.status_code})"
            
    except Exception as e:
        return f"AIç”Ÿæˆå‡ºé”™: {str(e)}"


def ai_generate_main_view(commodity_name: str, date_str: str, market_data: dict, news_list: list, 
                         professional_data: dict = None, technical_indicators: dict = None, market_date=None) -> str:
    """ä½¿ç”¨DeepSeek AIç”Ÿæˆä¸»è¦è§‚ç‚¹ï¼ˆä¸“ä¸šç‰ˆï¼š8å¤§ç»´åº¦åˆ†æï¼‰
    
    Args:
        commodity_name: å•†å“åç§°
        date_str: æ—¥æŠ¥ç”Ÿæˆæ—¥æœŸ
        market_data: å¸‚åœºæ•°æ®
        news_list: æ–°é—»åˆ—è¡¨
        professional_data: ä¸“ä¸šæ•°æ®
        technical_indicators: æŠ€æœ¯æŒ‡æ ‡
        market_date: å®é™…çš„å¸‚åœºå›é¡¾æ—¥æœŸï¼ˆäº¤æ˜“æ—¥ï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨è®¡ç®—å‰ä¸€å¤©
    """
    try:
        # å¦‚æœæä¾›äº†å®é™…çš„å¸‚åœºæ—¥æœŸï¼Œç›´æ¥ä½¿ç”¨ï¼›å¦åˆ™è®¡ç®—å‰ä¸€å¤©
        if market_date is None:
            report_date = datetime.strptime(date_str, '%Y-%m-%d')
            market_date = (report_date - timedelta(days=1)).strftime('%Y-%m-%d')
        elif isinstance(market_date, datetime):
            market_date = market_date.strftime('%Y-%m-%d')
        # å‡†å¤‡å•†å“æ–°é—»æ‘˜è¦ï¼ˆä½¿ç”¨ä¸Šæ ‡å¼•ç”¨æ ¼å¼ï¼‰
        news_summary = ""
        ref_index = 1
        for i, news in enumerate(news_list[:10], 1):
            news_summary += f"[{ref_index}] {news['title']}\n"
            if news.get('content'):
                content = news['content'][:150].replace('...', '').replace('â€¦', '')
                news_summary += f"    {content}\n"
            news_summary += f"    æ¥æºï¼š{news.get('source', 'æœªçŸ¥')} | æ—¥æœŸï¼š{news.get('date', 'N/A')} | URLï¼š{news.get('url', 'æ— ')}\n"
            ref_index += 1
        
        if not news_summary:
            news_summary = "æš‚æ— æœ€æ–°å•†å“æ–°é—»æ•°æ®"
        
        # å‡†å¤‡æŠ€æœ¯æŒ‡æ ‡æ‘˜è¦
        tech_summary = "\nã€æŠ€æœ¯åˆ†ææŒ‡æ ‡ã€‘\n"
        if technical_indicators and any(technical_indicators.values()):
            tech_summary += f"å½“å‰ä»·æ ¼ï¼š{technical_indicators.get('current_price', 'N/A')}å…ƒ\n"
            tech_summary += f"MA5ï¼š{technical_indicators.get('ma5', 'N/A')}å…ƒ\n"
            tech_summary += f"MA10ï¼š{technical_indicators.get('ma10', 'N/A')}å…ƒ\n"
            tech_summary += f"MA20ï¼š{technical_indicators.get('ma20', 'N/A')}å…ƒ\n"
            tech_summary += f"MACDï¼š{technical_indicators.get('macd', 'N/A')} (ä¿¡å·çº¿ï¼š{technical_indicators.get('macd_signal', 'N/A')})\n"
            tech_summary += f"RSI(14)ï¼š{technical_indicators.get('rsi', 'N/A')}\n"
            tech_summary += f"å¸ƒæ—å¸¦ï¼šä¸Šè½¨{technical_indicators.get('upper_band', 'N/A')}å…ƒ / ä¸‹è½¨{technical_indicators.get('lower_band', 'N/A')}å…ƒ\n"
            tech_summary += f"ä»·æ ¼ä½ç½®ï¼š{technical_indicators.get('price_position', 'N/A')}\n"
        else:
            tech_summary += "æŠ€æœ¯æŒ‡æ ‡æ•°æ®ä¸è¶³\n"
        
        # å‡†å¤‡8å¤§ç»´åº¦ä¸“ä¸šæ•°æ®æ‘˜è¦
        professional_summary = "\nã€ä¸“ä¸šç»´åº¦æ•°æ®ã€‘\n"
        if professional_data and len(professional_data) > 0:
            for category, data_list in professional_data.items():
                professional_summary += f"\nâ—† {category}ï¼š\n"
                for data in data_list[:2]:  # æ¯ä¸ªç»´åº¦æœ€å¤š2æ¡
                    professional_summary += f"[{ref_index}] {data.get('title', '')}\n"
                    if data.get('content'):
                        content = data['content'][:100].replace('...', '').replace('â€¦', '')
                        professional_summary += f"    {content}\n"
                    professional_summary += f"    æ¥æºï¼š{data.get('source', 'æœªçŸ¥')} | URLï¼š{data.get('url', 'æ— ')}\n"
                    ref_index += 1
        else:
            professional_summary += "æš‚æ— ä¸“ä¸šç»´åº¦æ•°æ®\n"
        
        prompt = f"""
ä½ æ˜¯èµ„æ·±æœŸè´§åˆ†æå¸ˆï¼Œæ­£åœ¨ä¸º{date_str}æ’°å†™{commodity_name}æœŸè´§çš„ä¸“ä¸šå¸‚åœºåˆ†ææŠ¥å‘Šã€‚

ã€é‡è¦çº¦æŸã€‘
1. âš ï¸ æ‰€æœ‰åˆ†æå¿…é¡»åŸºäºä¸‹æ–¹æä¾›çš„çœŸå®æ•°æ®ï¼Œä¸¥ç¦ç¼–é€ ä»»ä½•ä¿¡æ¯
2. âš ï¸ å¦‚æœæŸé¡¹æ•°æ®æ˜¾ç¤ºä¸º"N/A"æˆ–"æš‚æ— "ï¼Œä¸è¦æ¨æµ‹æˆ–ç¼–é€ è¯¥æ•°æ®
3. âš ï¸ å¼•ç”¨çš„æ‰€æœ‰ä»·æ ¼ã€æ¶¨è·Œå¹…ã€æŒ‡æ ‡å€¼å¿…é¡»ä¸æä¾›çš„æ•°æ®å®Œå…¨ä¸€è‡´
4. âš ï¸ æŠ¥å‘Šæ—¥æœŸä¸º{date_str}ï¼Œä¸è¦ä½¿ç”¨å…¶ä»–æ—¥æœŸçš„ä¿¡æ¯
5. âš ï¸ å¿…é¡»åŸºäº8å¤§åˆ†æç»´åº¦è¿›è¡Œç³»ç»Ÿæ€§åˆ†æ

ã€ä¸€ã€ä»·æ ¼æ•°æ® - {date_str}ã€‘
æ—¥ç›˜æ”¶ç›˜ä»·ï¼š{market_data.get('close', 'N/A')}å…ƒ
æ—¥ç›˜æ¶¨è·Œå¹…ï¼š{market_data.get('change_pct', 'N/A')}%
å¤œç›˜æ”¶ç›˜ä»·ï¼š{market_data.get('night_close', 'N/A')}å…ƒ
å¤œç›˜æ¶¨è·Œå¹…ï¼š{market_data.get('night_change_pct', 'N/A')}%
æœ€é«˜ä»·ï¼š{market_data.get('high', 'N/A')}å…ƒ
æœ€ä½ä»·ï¼š{market_data.get('low', 'N/A')}å…ƒ
{tech_summary}

ã€äºŒã€å¸‚åœºæ–°é—»èµ„è®¯ã€‘
{news_summary}
{professional_summary}

ã€æ’°å†™è¦æ±‚ - 8å¤§ç»´åº¦ä¸“ä¸šåˆ†ææ¡†æ¶ã€‘â­æ ¸å¿ƒ

è¯·æ’°å†™ä¸€æ®µ400-500å­—çš„æ·±åº¦ä¸»è¦è§‚ç‚¹ï¼Œå¿…é¡»åŒ…å«ä»¥ä¸‹ç»“æ„å’Œç»´åº¦ï¼š

**ç»“æ„æ¡†æ¶ï¼š**
ã€æ—©ç›˜èšç„¦ã€‘ï¼ˆ120å­—ï¼‰
- ä»Šæ—¥æœ€æ–°æ¶ˆæ¯åˆ†æ
- å¼€ç›˜å‰çªå‘äº‹ä»¶å½±å“è¯„ä¼°
- ä¸æ˜¨æ—¥è¡Œæƒ…çš„å…³è”åˆ†æ

ã€æ·±åº¦åˆ†æã€‘ï¼ˆ220å­—ï¼‰- â­æ ¸å¿ƒï¼Œå¿…é¡»è¦†ç›–8å¤§ç»´åº¦
1. **æŠ€æœ¯é¢**ï¼šå‡çº¿ç³»ç»Ÿã€RSIä¿¡å·ã€æ”¯æ’‘å‹åŠ›ä½
2. **åŸºæœ¬é¢**ï¼šä¾›éœ€æ ¼å±€ã€åº“å­˜ä»“å•ã€åŸºå·®èµ°åŠ¿[å¼•ç”¨æ•°æ®]
3. **èµ„é‡‘é¢**ï¼šæŒä»“å¸­ä½ã€ä¸»åŠ›åŠ¨å‘[å¼•ç”¨æ•°æ®]
4. **äº§ä¸šé“¾**ï¼šä¸Šä¸‹æ¸¸ä»·æ ¼ã€åˆ©æ¶¦[å¼•ç”¨æ•°æ®]
5. **æ”¿ç­–é¢**ï¼šæœ€æ–°æ”¿ç­–å½±å“[å¼•ç”¨æ•°æ®]
6. **è¿›å‡ºå£**ï¼šæµ·å…³æ•°æ®[å¼•ç”¨æ•°æ®]
7. **å¸‚åœºæƒ…ç»ª**ï¼šæ–°é—»èˆ†æƒ…å˜åŒ–[å¼•ç”¨æ•°æ®]
8. **é£é™©å› ç´ **ï¼šå…³é”®é£é™©ç‚¹[å¼•ç”¨æ•°æ®]

ã€ä»Šæ—¥è§‚ç‚¹åŠæ“ä½œå»ºè®®ã€‘ï¼ˆ160å­—ï¼‰
- **ç»¼åˆåˆ¤æ–­**ï¼šå¤šç©ºæ–¹å‘ã€ä»·æ ¼åŒºé—´ã€å…³é”®å˜é‡ã€é€»è¾‘é“¾
- **æ“ä½œç­–ç•¥**ï¼šåšå¤š/åšç©ºæ–¹å‘ã€å»ºä»“åŒºé—´ã€æ­¢æŸä½ã€æ­¢ç›ˆä½ã€ä»“ä½å»ºè®®ã€æ³¨æ„äº‹é¡¹

ã€å¼•ç”¨æ ¼å¼è¦æ±‚ã€‘â­é‡è¦
- æ‰€æœ‰äº‹å®æ€§é™ˆè¿°å¿…é¡»ä½¿ç”¨ä¸Šæ ‡[1][2][3]
- æŠ€æœ¯æŒ‡æ ‡æ•°æ®ç›´æ¥å¼•ç”¨ï¼Œæ— éœ€ä¸Šæ ‡
- æ¯ä¸ªç»´åº¦éƒ½è¦æœ‰å…·ä½“åˆ†æ

âš ï¸ å†æ¬¡å¼ºè°ƒï¼š
1. ä¸è¦åŒ…å«ã€è¡Œæƒ…å›é¡¾ã€‘ï¼ˆå·²åœ¨"ä¸€ã€å¸‚åœºèµ°åŠ¿å›é¡¾"ä¸­æè¿°ï¼‰
2. ã€ä»Šæ—¥è§‚ç‚¹ã€‘å’Œã€æ“ä½œå»ºè®®ã€‘åˆå¹¶ä¸ºã€ä»Šæ—¥è§‚ç‚¹åŠæ“ä½œå»ºè®®ã€‘
3. å¿…é¡»è¦†ç›–8å¤§åˆ†æç»´åº¦
4. è¯­è¨€ä¸“ä¸šã€é€»è¾‘ä¸¥å¯†ã€ç»“è®ºæ˜ç¡®

è¯·ç›´æ¥è¾“å‡ºè§‚ç‚¹æ–‡æœ¬ï¼ˆæŒ‰ã€æ—©ç›˜èšç„¦ã€‘ã€æ·±åº¦åˆ†æã€‘ã€ä»Šæ—¥è§‚ç‚¹åŠæ“ä½œå»ºè®®ã€‘ä¸‰éƒ¨åˆ†ç»“æ„è¾“å‡ºï¼‰ï¼š
"""
        
        headers = {
            "Authorization": f"Bearer {DEEPSEEK_API_KEY}",
            "Content-Type": "application/json"
        }
        
        data = {
            "model": "deepseek-chat",
            "messages": [{"role": "user", "content": prompt}],
            "temperature": 0.2,  # é™ä½æ¸©åº¦ï¼Œæé«˜å‡†ç¡®æ€§
            "max_tokens": 1200  # å¢åŠ tokenæ•°ï¼Œæ”¯æŒ300-400å­—çš„8ç»´åº¦åˆ†æ
        }
        
        response = requests.post(DEEPSEEK_API_URL, headers=headers, json=data, timeout=90)
        
        print(f"[DEBUG] AIç”Ÿæˆä¸»è¦è§‚ç‚¹ - çŠ¶æ€ç : {response.status_code}")
        
        if response.status_code == 200:
            result = response.json()
            content = result['choices'][0]['message']['content'].strip()
            print(f"[DEBUG] AIç”Ÿæˆä¸»è¦è§‚ç‚¹ - è¿”å›å†…å®¹é•¿åº¦: {len(content)}å­—ç¬¦")
            print(f"[DEBUG] AIç”Ÿæˆä¸»è¦è§‚ç‚¹ - å‰100å­—ç¬¦: {content[:100]}")
            return content
        else:
            error_msg = f"AIç”Ÿæˆå¤±è´¥ (çŠ¶æ€ç : {response.status_code})"
            print(f"[ERROR] AIç”Ÿæˆä¸»è¦è§‚ç‚¹ - {error_msg}")
            if response.status_code == 401:
                print(f"[ERROR] APIå¯†é’¥æ— æ•ˆæˆ–è¿‡æœŸ")
            elif response.status_code == 429:
                print(f"[ERROR] APIè°ƒç”¨é¢‘ç‡è¿‡é«˜æˆ–é¢åº¦ç”¨å®Œ")
            return error_msg
            
    except Exception as e:
        error_msg = f"AIç”Ÿæˆå‡ºé”™: {str(e)}"
        print(f"[ERROR] AIç”Ÿæˆä¸»è¦è§‚ç‚¹ - {error_msg}")
        import traceback
        print(f"[ERROR] è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        return error_msg


# ============ å“ç§æ˜ å°„ï¼ˆç”¨äºè¾“å…¥æç¤ºï¼‰============
COMMODITY_EXAMPLES = {
    "é“œ": "CU2501",
    "é“": "AL2501",
    "é”Œ": "ZN2501",
    "é“…": "PB2501",
    "é•": "NI2501",
    "é”¡": "SN2501",
    "èºçº¹é’¢": "RB2501",
    "çƒ­å·": "HC2501",
    "é“çŸ¿çŸ³": "I2501",
    "ç„¦ç‚­": "J2501",
    "ç„¦ç…¤": "JM2501",
    "åŸæ²¹": "SC2501",
    "PTA": "TA2501",
    "ç”²é†‡": "MA2501",
    "è±†ç²•": "M2501",
    "è±†æ²¹": "Y2501",
    "ç‰ç±³": "C2501",
    "ç™½ç³–": "SR2501",
    "æ£‰èŠ±": "CF2501",
    "é»„é‡‘": "AU2512",
    "ç™½é“¶": "AG2512"
}


# åˆ›å»ºæ–‡ä»¶å¤¹å’Œæ–‡æ¡£ä¿å­˜è·¯å¾„
def create_folder_and_doc_path(custom_date):
    # è‡ªåŠ¨è·å–å½“å‰ç”¨æˆ·çš„æ¡Œé¢è·¯å¾„
    try:
        # å°è¯•è·å–æ¡Œé¢è·¯å¾„
        desktop = os.path.join(os.path.expanduser("~"), "Desktop")
        # å¦‚æœæ¡Œé¢ä¸å­˜åœ¨ï¼ˆä¸­æ–‡ç³»ç»Ÿå¯èƒ½æ˜¯"æ¡Œé¢"ï¼‰ï¼Œå°è¯•ä¸­æ–‡è·¯å¾„
        if not os.path.exists(desktop):
            desktop = os.path.join(os.path.expanduser("~"), "æ¡Œé¢")
        # å¦‚æœè¿˜æ˜¯ä¸å­˜åœ¨ï¼Œå°±ç”¨ç”¨æˆ·ä¸»ç›®å½•
        if not os.path.exists(desktop):
            desktop = os.path.expanduser("~")
    except:
        # å¦‚æœå‡ºé”™ï¼Œä½¿ç”¨å½“å‰ç›®å½•
        desktop = "."

    base_path = os.path.join(desktop, "æœŸè´§æ—¥æŠ¥")
    folder_path = os.path.join(base_path, f"æœŸè´§æ—¥æŠ¥_{custom_date}")

    # åˆ›å»ºç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    try:
        if not os.path.exists(folder_path):
            os.makedirs(folder_path, exist_ok=True)
    except Exception as e:
        # å¦‚æœåˆ›å»ºå¤±è´¥ï¼Œä½¿ç”¨å½“å‰ç›®å½•
        folder_path = f"æœŸè´§æ—¥æŠ¥_{custom_date}"
        if not os.path.exists(folder_path):
            os.makedirs(folder_path, exist_ok=True)

    base_filename = "æœŸè´§æ—¥æŠ¥"
    filename = f"{base_filename}_{custom_date}.docx"
    doc_path = os.path.join(folder_path, filename)
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨ä¸”è¢«å ç”¨ï¼Œå¦‚æœæ˜¯åˆ™ç”Ÿæˆæ–°æ–‡ä»¶å
    if os.path.exists(doc_path):
        counter = 1
        while True:
            new_filename = f"{base_filename}_{custom_date}_{counter}.docx"
            new_doc_path = os.path.join(folder_path, new_filename)
            if not os.path.exists(new_doc_path):
                doc_path = new_doc_path
                break
            # å°è¯•æ‰“å¼€æ–‡ä»¶ï¼Œå¦‚æœå¯ä»¥æ‰“å¼€è¯´æ˜æ²¡è¢«å ç”¨ï¼Œå¯ä»¥è¦†ç›–
            try:
                with open(new_doc_path, 'a'):
                    doc_path = new_doc_path
                    break
            except:
                counter += 1
                if counter > 10:  # æœ€å¤šå°è¯•10æ¬¡
                    doc_path = new_doc_path
                    break
    
    return doc_path, folder_path


# è®¾ç½®ä¸“ä¸šæ–‡æ¡£æ ·å¼
def set_professional_doc_style(doc):
    """è®¾ç½®ä¸“ä¸šæœŸè´§æ—¥æŠ¥æ ·å¼"""
    # è®¾ç½®æ­£æ–‡æ ·å¼
    normal = doc.styles["Normal"]
    normal.font.name = "Times New Roman"
    normal._element.rPr.rFonts.set(qn("w:eastAsia"), "å®‹ä½“")  # ä½¿ç”¨å®‹ä½“
    normal.font.size = Pt(12)
    
    # è®¾ç½®æ®µè½æ ¼å¼
    normal.paragraph_format.space_after = Pt(6)
    normal.paragraph_format.line_spacing = 1.2


# æ™ºèƒ½è·å–æœ€è¿‘çš„äº¤æ˜“æ—¥
def get_last_trading_day(symbol, report_date, max_days_back=7):
    """æ™ºèƒ½è·å–æœ€è¿‘çš„äº¤æ˜“æ—¥
    
    ä»report_dateå‘å‰æŸ¥æ‰¾ï¼Œæ‰¾åˆ°æœ€è¿‘ä¸€ä¸ªæœ‰äº¤æ˜“æ•°æ®çš„æ—¥æœŸ
    é¿å…å‘¨æœ«ã€èŠ‚å‡æ—¥å¯¼è‡´çš„æ•°æ®ç¼ºå¤±é—®é¢˜
    
    Args:
        symbol: åˆçº¦ä»£ç 
        report_date: æ—¥æŠ¥ç”Ÿæˆæ—¥æœŸ
        max_days_back: æœ€å¤šå‘å‰æŸ¥æ‰¾çš„å¤©æ•°
    
    Returns:
        æœ€è¿‘çš„äº¤æ˜“æ—¥æ—¥æœŸå¯¹è±¡ï¼Œå¦‚æœæ‰¾ä¸åˆ°åˆ™è¿”å›None
    """
    st.info(f"ğŸ” æ­£åœ¨æŸ¥æ‰¾æœ€è¿‘çš„äº¤æ˜“æ—¥...")
    
    for days_back in range(1, max_days_back + 1):
        check_date = report_date - timedelta(days=days_back)
        check_date_str = check_date.strftime('%Y-%m-%d')
        
        # å°è¯•è·å–è¯¥æ—¥æœŸçš„æ•°æ®
        try:
            start_time = check_date_str + ' 09:00:00'
            end_time = check_date_str + ' 15:00:00'
            
            df = ak.futures_zh_minute_sina(symbol=symbol, period="1")
            if not df.empty:
                df['datetime'] = pd.to_datetime(df['datetime'])
                day_data = df[(df['datetime'] >= start_time) & (df['datetime'] <= end_time)]
                
                if not day_data.empty:
                    if days_back > 1:
                        weekday = check_date.strftime('%A')
                        weekday_cn = {'Monday': 'å‘¨ä¸€', 'Tuesday': 'å‘¨äºŒ', 'Wednesday': 'å‘¨ä¸‰', 
                                    'Thursday': 'å‘¨å››', 'Friday': 'å‘¨äº”', 'Saturday': 'å‘¨å…­', 'Sunday': 'å‘¨æ—¥'}
                        st.success(f"âœ… æ‰¾åˆ°äº¤æ˜“æ—¥ï¼š{check_date_str}ï¼ˆå‘å‰{days_back}å¤©ï¼Œ{weekday_cn.get(weekday, weekday)}ï¼‰")
                    else:
                        st.success(f"âœ… æ‰¾åˆ°äº¤æ˜“æ—¥ï¼š{check_date_str}")
                    return check_date
            
            time.sleep(0.1)  # é¿å…è¯·æ±‚è¿‡å¿«
        except Exception as e:
            continue
    
    st.warning(f"âš ï¸ æœªæ‰¾åˆ°æœ‰æ•ˆäº¤æ˜“æ—¥ï¼ˆå·²å›æº¯{max_days_back}å¤©ï¼‰")
    return None


# è·å–å½“å¤©è¡Œæƒ…æ¦‚è¿°æ•°æ®ï¼ˆæ™ºèƒ½æŸ¥æ‰¾æœ€è¿‘äº¤æ˜“æ—¥ï¼‰
def get_market_trend_data(symbol, custom_date):
    """è·å–å¸‚åœºèµ°åŠ¿æ•°æ®ï¼ˆåŒ…å«ç™½å¤©ç›˜å’Œå¤œç›˜ï¼‰
    
    å¯¹äºæ—¥æŠ¥ç”Ÿæˆæ—¥æœŸï¼ˆcustom_dateï¼‰ï¼Œè·å–æœ€è¿‘äº¤æ˜“æ—¥çš„å®Œæ•´äº¤æ˜“æ•°æ®ï¼š
    - æ—¥ç›˜ï¼šäº¤æ˜“æ—¥ 9:00-15:00
    - å¤œç›˜ï¼šäº¤æ˜“æ—¥ 21:00 åˆ°æ¬¡æ—¥å‡Œæ™¨ï¼ˆè¦†ç›–æ‰€æœ‰å“ç§ï¼Œæœ€æ™šåˆ°02:30ï¼‰
    
    Returns:
        day_description: æ—¥ç›˜æè¿°
        night_description: å¤œç›˜æè¿°
        filtered_data: è¿‡æ»¤åçš„æ•°æ®æ¡†
        market_data_dict: å¸‚åœºæ•°æ®å­—å…¸
        market_date: å®é™…çš„äº¤æ˜“æ—¥æ—¥æœŸå¯¹è±¡
    """
    try:
        # æ—¥æŠ¥ç”Ÿæˆæ—¥æœŸ
        report_date = custom_date
        
        # æ™ºèƒ½æŸ¥æ‰¾æœ€è¿‘çš„äº¤æ˜“æ—¥ï¼ˆè€Œä¸æ˜¯ç®€å•çš„å‰ä¸€å¤©ï¼‰
        market_date = get_last_trading_day(symbol, report_date)
        
        if market_date is None:
            st.error("âŒ æ— æ³•æ‰¾åˆ°æœ‰æ•ˆçš„äº¤æ˜“æ—¥æ•°æ®")
            return "", "", pd.DataFrame(), {}, None
        
        # è·å–æ•°æ®èŒƒå›´ï¼šä»äº¤æ˜“æ—¥çš„9:00åˆ°æ¬¡æ—¥å‡Œæ™¨03:00ï¼ˆè¦†ç›–æ‰€æœ‰å¤œç›˜å“ç§ï¼‰
        next_day = market_date + timedelta(days=1)
        start_time = market_date.strftime('%Y-%m-%d') + ' 09:00:00'
        end_time = next_day.strftime('%Y-%m-%d') + ' 03:00:00'
        
        df = ak.futures_zh_minute_sina(symbol=symbol, period="1")
        
        if df.empty:
            st.error(f"âš ï¸ akshareæœªè¿”å›ä»»ä½•æ•°æ®ï¼Œå¯èƒ½åŸå› ï¼šåˆçº¦ä»£ç {symbol}ä¸å­˜åœ¨æˆ–æ ¼å¼é”™è¯¯")
            return "", "", pd.DataFrame(), {}, None
        
        df['datetime'] = pd.to_datetime(df['datetime'])
        filtered_data = df[(df['datetime'] >= start_time) & (df['datetime'] <= end_time)]
        
        if filtered_data.empty:
            st.error(f"âš ï¸ æŒ‡å®šæ—¶é—´èŒƒå›´å†…æ— æ•°æ®")
            return "", "", pd.DataFrame(), {}, None

        # æ—¥ç›˜æ•°æ®ï¼šäº¤æ˜“æ—¥ 9:00-15:00
        day_start = market_date.strftime('%Y-%m-%d') + ' 09:00:00'
        day_end = market_date.strftime('%Y-%m-%d') + ' 15:00:00'
        day_data = filtered_data[(filtered_data['datetime'] >= day_start) & (filtered_data['datetime'] <= day_end)]
        
        if day_data.empty:
            st.error(f"âš ï¸ æ—¥ç›˜æ—¶æ®µï¼ˆ{day_start} è‡³ {day_end}ï¼‰æ— æ•°æ®")
            return "", "", pd.DataFrame(), {}, None
        
        # è·å–å¼€ç›˜ä»·å’Œæ”¶ç›˜ä»·
        day_open_price = day_data.iloc[0]['open']
        day_close_price = day_data.iloc[-1]['close']

        high_price = day_data['high'].max()
        low_price = day_data['low'].min()
        price_change = day_close_price - day_open_price
        price_change_percentage = (price_change / day_open_price) * 100
        trend = "ä¸Šæ¶¨" if price_change > 0 else "ä¸‹è·Œ" if price_change < 0 else "æŒå¹³"
        
        day_description = (
            f"{market_date.strftime('%Y-%m-%d')}æ—¥{symbol}ä¸»åŠ›åˆçº¦å¼€ç›˜ä»·ä¸º{day_open_price}å…ƒ/å¨ï¼Œæœ€é«˜ä»·ä¸º{high_price}å…ƒ/å¨ï¼Œ"
            f"æœ€ä½ä»·ä¸º{low_price}å…ƒ/å¨ï¼Œæ”¶ç›˜ä»·ä¸º{day_close_price}å…ƒ/å¨ï¼Œè¾ƒå‰ä¸€æ—¥{trend}äº†"
            f"{abs(price_change):.2f}å…ƒ/å¨ï¼Œæ¶¨è·Œå¹…ä¸º{price_change_percentage:.2f}%ã€‚"
        )

        # å¤œç›˜æ•°æ®ï¼šäº¤æ˜“æ—¥ 21:00 åˆ°æ¬¡æ—¥å‡Œæ™¨03:00ï¼ˆè¦†ç›–æ‰€æœ‰å“ç§ï¼‰
        night_start_time = market_date.strftime('%Y-%m-%d') + ' 21:00:00'
        night_end_time = next_day.strftime('%Y-%m-%d') + ' 03:00:00'
        night_data = df[(df['datetime'] >= night_start_time) & (df['datetime'] <= night_end_time)]
        
        night_description = ""
        market_data_dict = {
            'open': day_open_price,
            'close': day_close_price,
            'high': high_price,
            'low': low_price,
            'change': price_change,
            'change_pct': price_change_percentage
        }
        
        if night_data.empty:
            night_description = "å¤œç›˜æ•°æ®ä¸å¯ç”¨ã€‚"
            market_data_dict.update({
                'night_open': 'N/A',
                'night_close': 'N/A',
                'night_change': 'N/A',
                'night_change_pct': 'N/A'
            })
        else:
            night_open_price = night_data.iloc[0]['open']
            night_close_price = night_data.iloc[-1]['close']
            night_price_change = night_close_price - night_open_price
            night_price_change_percentage = (night_price_change / night_open_price) * 100
            night_trend = "ä¸Šæ¶¨" if night_price_change > 0 else "ä¸‹è·Œ" if night_price_change < 0 else "æŒå¹³"
            night_description = (
                f"å¤œç›˜èµ°åŠ¿ï¼šå¼€ç›˜ä»·ä¸º{night_open_price}å…ƒ/å¨ï¼Œæ”¶ç›˜ä»·ä¸º{night_close_price}å…ƒ/å¨ï¼Œè¾ƒå¼€ç›˜{night_trend}äº†"
                f"{abs(night_price_change):.2f}å…ƒ/å¨ï¼Œæ¶¨è·Œå¹…ä¸º{night_price_change_percentage:.2f}%ã€‚"
            )
            market_data_dict.update({
                'night_open': night_open_price,
                'night_close': night_close_price,
                'night_change': night_price_change,
                'night_change_pct': night_price_change_percentage
            })

        # è¿”å›æ—¥ç›˜æè¿°ã€å¤œç›˜æè¿°ã€è¿‡æ»¤åçš„æ•°æ®æ¡†ã€å¸‚åœºæ•°æ®å­—å…¸ã€å®é™…çš„äº¤æ˜“æ—¥æ—¥æœŸ
        return day_description, night_description, filtered_data, market_data_dict, market_date
    except Exception as e:
        st.error(f"âŒ æ•°æ®è·å–å¼‚å¸¸: {str(e)}")
        return f"è·å–å¸‚åœºèµ°åŠ¿æ•°æ®å¤±è´¥: {e}", "", pd.DataFrame(), {}, None


# åˆ›å»ºKçº¿å›¾
def create_k_line_chart(data, symbol, folder_path):
    if data.empty:
        print("æ•°æ®ä¸ºç©ºï¼Œæ— æ³•ç”ŸæˆKçº¿å›¾ã€‚")
        return None
    
    # ä¸´æ—¶é‡ç½®matplotlibé…ç½®ï¼Œé¿å…Linuxå­—ä½“é”™è¯¯
    import matplotlib
    matplotlib.rcParams['font.family'] = 'sans-serif'
    matplotlib.rcParams['font.sans-serif'] = ['DejaVu Sans']
    matplotlib.rcParams['axes.unicode_minus'] = False
    
    data.set_index('datetime', inplace=True)
    data = data[['open', 'high', 'low', 'close']]
    data.columns = ['Open', 'High', 'Low', 'Close']
    fig, ax = plt.subplots(figsize=(10, 6))
    mpf.plot(data, type='candle', style='charles', ax=ax)
    k_line_chart_path = os.path.join(folder_path, 'k_line_chart.png')
    plt.savefig(k_line_chart_path, dpi=100)
    plt.close(fig)
    return k_line_chart_path


# è·å–æ–°é—»æ•°æ®ï¼ˆä½¿ç”¨å¢å¼ºæœç´¢å™¨ï¼‰
def get_news_data_enhanced(commodity_name: str, serper_key: str = None, target_dates: list = None):
    """ä½¿ç”¨å¢å¼ºæ–°é—»æœç´¢å™¨è·å–æ–°é—»ï¼ˆæ”¯æŒæŒ‡å®šæ—¥æœŸåˆ—è¡¨ï¼Œä½¿ç”¨ä¸Šæ ‡å¼•ç”¨æ ¼å¼ï¼‰"""
    try:
        searcher = EnhancedNewsSearcher()
        news_list = searcher.comprehensive_search(
            commodity=commodity_name,
            days_back=3,
            serper_key=serper_key,
            target_dates=target_dates
        )
        
        # æ ¼å¼åŒ–æ–°é—»ï¼Œæ˜¾ç¤ºæ‰€æœ‰æ–°é—»ï¼Œä½¿ç”¨ä¸Šæ ‡å¼•ç”¨æ ¼å¼[1][2][3]
        description = ""
        for i, news in enumerate(news_list, 1):
            # ä¼˜å…ˆä½¿ç”¨contentï¼Œå¦‚æœæ²¡æœ‰å°±ç”¨title
            if news.get('content'):
                # æ¸…ç†å†…å®¹ï¼Œå»é™¤å¤šä½™ç©ºæ ¼ã€æ¢è¡Œå’Œçœç•¥å·
                content = news['content'].strip().replace('...', '').replace('â€¦', '')
                # ä¸é™åˆ¶é•¿åº¦ï¼Œæ˜¾ç¤ºå®Œæ•´å†…å®¹ï¼ˆå¦‚æœå¤ªé•¿Wordä¼šè‡ªåŠ¨æ¢è¡Œï¼‰
            else:
                # å¦‚æœæ²¡æœ‰contentï¼Œå°±ç”¨title
                content = news['title']
            
            # æ ¼å¼ï¼šåºå·. å†…å®¹[ä¸Šæ ‡]
            description += f"{i}. {content}[{i}]\n\n"
        
        return description if description else "æš‚æ— ç›¸å…³æ–°é—»", news_list
    except Exception as e:
        return f"è·å–æ–°é—»æ•°æ®å¤±è´¥: {e}", []


# åˆ›å»ºæŠ¥å‘Šï¼ˆä¸“ä¸šç‰ˆï¼‰
def create_report_professional(custom_date_str, symbol, commodity_name, user_description, main_view, 
                               user_news_content=None, serper_key=None):
    """åˆ›å»ºä¸“ä¸šæœŸè´§æ—¥æŠ¥
    
    Args:
        custom_date_str: æ—¥æœŸå­—ç¬¦ä¸²
        symbol: åˆçº¦ä»£ç 
        commodity_name: å“ç§åç§°
        user_description: ç”¨æˆ·ç¼–è¾‘çš„è¡Œæƒ…æè¿°
        main_view: ç”¨æˆ·ç¼–è¾‘çš„ä¸»è¦è§‚ç‚¹
        user_news_content: ç”¨æˆ·ç¼–è¾‘çš„æ–°é—»èµ„è®¯å†…å®¹ï¼ˆå¯é€‰ï¼‰
        serper_key: Serper APIå¯†é’¥
    """
    custom_date = datetime.strptime(custom_date_str, '%Y-%m-%d')
    doc_path, folder_path = create_folder_and_doc_path(custom_date_str)
    
    # è·å–å¸‚åœºæ•°æ®ï¼ˆè¿”å›5ä¸ªå€¼ï¼šæè¿°ã€å¤œç›˜æè¿°ã€æ•°æ®æ¡†ã€æ•°æ®å­—å…¸ã€å®é™…äº¤æ˜“æ—¥ï¼‰
    market_trend_description, night_trend_description, market_data, market_data_dict, actual_market_date = get_market_trend_data(symbol=symbol, custom_date=custom_date)
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°äº¤æ˜“æ—¥ï¼Œè¿”å›None
    if actual_market_date is None:
        st.error("æ— æ³•ç”ŸæˆæŠ¥å‘Šï¼Œå› ä¸ºæœªæ‰¾åˆ°æœ‰æ•ˆçš„äº¤æ˜“æ—¥æ•°æ®ã€‚")
        return None
    
    # ç”Ÿæˆèµ„è®¯æ—¥æœŸåˆ—è¡¨ï¼šä»äº¤æ˜“æ—¥åˆ°æŠ¥å‘Šæ—¥æœŸçš„æ‰€æœ‰æ—¥æœŸ
    news_date_list = []
    current_date = actual_market_date
    while current_date <= custom_date:
        news_date_list.append(current_date.strftime('%Y-%m-%d'))
        current_date += timedelta(days=1)
    
    st.info(f"ğŸ“… å¸‚åœºå›é¡¾ï¼š{actual_market_date.strftime('%Y-%m-%d')}  |  èµ„è®¯æœç´¢ï¼š{actual_market_date.strftime('%Y-%m-%d')} è‡³ {custom_date_str}ï¼ˆå…±{len(news_date_list)}å¤©ï¼‰")
    
    if market_data.empty:
        st.error("æ— æ³•ç”ŸæˆæŠ¥å‘Šï¼Œå› ä¸ºå¸‚åœºæ•°æ®ä¸ºç©ºã€‚")
        return None
    
    # ä½¿ç”¨ç”¨æˆ·ç¼–è¾‘çš„æ–°é—»å†…å®¹ï¼Œå¦‚æœæ²¡æœ‰åˆ™è‡ªåŠ¨è·å–
    if user_news_content and user_news_content.strip():
        news_description = user_news_content
        # ä»ç”¨æˆ·å†…å®¹ä¸­æå–å¼•ç”¨ï¼Œç”¨äºé™„å½•ï¼ˆç®€åŒ–å¤„ç†ï¼Œæå–æ‰€æœ‰[æ•°å­—]æ ¼å¼çš„å¼•ç”¨ï¼‰
        import re
        ref_numbers = re.findall(r'\[(\d+)\]', news_description)
        # å¦‚æœæœ‰å¼•ç”¨ï¼Œè·å–åŸå§‹æ–°é—»åˆ—è¡¨ä»¥æ„å»ºé™„å½•
        if ref_numbers and serper_key:
            _, news_list = get_news_data_enhanced(commodity_name, serper_key, news_date_list)
        else:
            news_list = []
    else:
        # ä½¿ç”¨å¢å¼ºæ–°é—»æœç´¢å™¨ï¼ˆä¼ å…¥æ—¥æœŸåˆ—è¡¨ï¼Œä¿å­˜æ–°é—»åˆ—è¡¨ç”¨äºé™„å½•ï¼‰
        news_description, news_list = get_news_data_enhanced(commodity_name, serper_key, news_date_list)
    
    k_line_chart_path = create_k_line_chart(market_data, symbol, folder_path)

    doc = Document()
    set_professional_doc_style(doc)

    # === 1. æ·»åŠ æ ‡é¢˜ ===
    title = doc.add_paragraph()
    title.alignment = WD_ALIGN_PARAGRAPH.CENTER
    title_run = title.add_run(f"æœŸè´§æ—¥æŠ¥")
    title_run.font.name = 'é»‘ä½“'
    title_run._element.rPr.rFonts.set(qn('w:eastAsia'), 'é»‘ä½“')
    title_run.font.size = Pt(18)
    title_run.font.bold = True
    title_run.font.color.rgb = RGBColor(31, 73, 125)

    # === 2. æ·»åŠ å“ç§å’Œæ—¥æœŸ ===
    subtitle = doc.add_paragraph()
    subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
    subtitle_run = subtitle.add_run(f"{commodity_name} | {custom_date_str}")
    subtitle_run.font.size = Pt(14)
    subtitle_run.font.bold = True
    
    # æ·»åŠ åˆ†éš”çº¿
    doc.add_paragraph("_" * 50)

    # === 3. å¸‚åœºèµ°åŠ¿å›é¡¾ ===
    market_heading = doc.add_paragraph()
    market_heading_run = market_heading.add_run("ä¸€ã€å¸‚åœºèµ°åŠ¿å›é¡¾")
    market_heading_run.font.name = 'é»‘ä½“'
    market_heading_run._element.rPr.rFonts.set(qn('w:eastAsia'), 'é»‘ä½“')
    market_heading_run.font.size = Pt(14)
    market_heading_run.font.bold = True
    
    # æ·»åŠ Kçº¿å›¾
    if k_line_chart_path:
        doc.add_picture(k_line_chart_path, width=Inches(6))
        # å›¾ç‰‡è¯´æ˜
        pic_caption = doc.add_paragraph()
        pic_caption.alignment = WD_ALIGN_PARAGRAPH.CENTER
        pic_caption_run = pic_caption.add_run(f"å›¾1ï¼š{commodity_name}æ—¥å†…èµ°åŠ¿å›¾")
        pic_caption_run.font.size = Pt(10)
        pic_caption_run.italic = True
    
    # è¡Œæƒ…æè¿°
    market_content = doc.add_paragraph()
    market_content.add_run(user_description)

    # === 4. ä¸»è¦è§‚ç‚¹ ===
    main_view_heading = doc.add_paragraph()
    main_view_heading_run = main_view_heading.add_run("äºŒã€ä¸»è¦è§‚ç‚¹")
    main_view_heading_run.font.name = 'é»‘ä½“'
    main_view_heading_run._element.rPr.rFonts.set(qn('w:eastAsia'), 'é»‘ä½“')
    main_view_heading_run.font.size = Pt(14)
    main_view_heading_run.font.bold = True
    
    main_view_content = doc.add_paragraph()
    main_view_content.add_run(main_view)

    # === 5. å¸‚åœºèµ„è®¯ ===
    news_heading = doc.add_paragraph()
    news_heading_run = news_heading.add_run("ä¸‰ã€å¸‚åœºèµ„è®¯")
    news_heading_run.font.name = 'é»‘ä½“'
    news_heading_run._element.rPr.rFonts.set(qn('w:eastAsia'), 'é»‘ä½“')
    news_heading_run.font.size = Pt(14)
    news_heading_run.font.bold = True
    
    news_content = doc.add_paragraph()
    news_content.add_run(news_description)

    # === 6. é™„å½•ï¼šå‚è€ƒæ–‡çŒ® ===
    doc.add_paragraph()  # ç©ºè¡Œ
    doc.add_paragraph("_" * 50)
    
    appendix_heading = doc.add_paragraph()
    appendix_heading_run = appendix_heading.add_run("é™„å½•ï¼šå‚è€ƒæ–‡çŒ®")
    appendix_heading_run.font.name = 'é»‘ä½“'
    appendix_heading_run._element.rPr.rFonts.set(qn('w:eastAsia'), 'é»‘ä½“')
    appendix_heading_run.font.size = Pt(12)
    appendix_heading_run.font.bold = True
    
    # åˆ—å‡ºæ‰€æœ‰æ–°é—»å¼•ç”¨ï¼ˆä¸é™åˆ¶æ•°é‡ï¼Œæ˜¾ç¤ºå…¨éƒ¨ï¼‰
    for i, news in enumerate(news_list, 1):
        ref_para = doc.add_paragraph()
        ref_para.paragraph_format.left_indent = Inches(0.3)
        
        # å¼•ç”¨åºå·
        ref_run = ref_para.add_run(f"[{i}] ")
        ref_run.font.bold = True
        ref_run.font.size = Pt(10)
        
        # æ–°é—»æ ‡é¢˜
        title_run = ref_para.add_run(news.get('title', 'æ— æ ‡é¢˜'))
        title_run.font.size = Pt(10)
        
        # æ¥æºå’Œæ—¥æœŸ
        source_run = ref_para.add_run(f"\n    æ¥æºï¼š{news.get('source', 'æœªçŸ¥')} | æ—¥æœŸï¼š{news.get('date', 'N/A')}")
        source_run.font.size = Pt(9)
        source_run.italic = True
        
        # URLé“¾æ¥
        if news.get('url'):
            url_run = ref_para.add_run(f"\n    é“¾æ¥ï¼š{news.get('url', 'æ— ')}")
            url_run.font.size = Pt(9)
            url_run.font.color.rgb = RGBColor(0, 0, 255)  # è“è‰²é“¾æ¥

    # === 7. æŠ¥å‘Šè¯´æ˜ ===
    doc.add_paragraph()  # ç©ºè¡Œ
    doc.add_paragraph("_" * 50)
    
    disclaimer = doc.add_paragraph()
    disclaimer_run = disclaimer.add_run("æŠ¥å‘Šè¯´æ˜")
    disclaimer_run.font.size = Pt(10)
    disclaimer_run.font.bold = True
    
    disclaimer_content = doc.add_paragraph()
    disclaimer_content_run = disclaimer_content.add_run(
        "æœ¬æŠ¥å‘ŠåŸºäºå…¬å¼€ä¿¡æ¯å’Œå¸‚åœºæ•°æ®ç¼–åˆ¶ï¼Œä»…ä¾›å‚è€ƒã€‚æœŸè´§å¸‚åœºå­˜åœ¨é£é™©ï¼ŒæŠ•èµ„éœ€è°¨æ…ã€‚"
        "æœ¬æŠ¥å‘Šä¸æ„æˆæŠ•èµ„å»ºè®®ï¼ŒæŠ•èµ„è€…åº”æ ¹æ®è‡ªèº«æƒ…å†µç‹¬ç«‹å†³ç­–å¹¶æ‰¿æ‹…é£é™©ã€‚"
    )
    disclaimer_content_run.font.size = Pt(9)
    disclaimer_content_run.italic = True
    
    # === 8. é¡µè„šä¿¡æ¯ ===
    footer_para = doc.add_paragraph()
    footer_para.alignment = WD_ALIGN_PARAGRAPH.RIGHT
    footer_run = footer_para.add_run(f"{custom_date_str}")
    footer_run.font.size = Pt(9)
    footer_run.italic = True

    # ä¿å­˜æ–‡æ¡£ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
    max_retries = 3
    for attempt in range(max_retries):
        try:
            doc.save(doc_path)
            return doc_path
        except PermissionError as e:
            if attempt < max_retries - 1:
                # å¦‚æœä¸æ˜¯æœ€åä¸€æ¬¡å°è¯•ï¼Œç”Ÿæˆæ–°çš„æ–‡ä»¶å
                import time
                time.sleep(0.5)  # ç­‰å¾…0.5ç§’
                base_path = os.path.dirname(doc_path)
                base_name = os.path.basename(doc_path).replace('.docx', '')
                new_doc_path = os.path.join(base_path, f"{base_name}_å‰¯æœ¬{attempt+1}.docx")
                doc_path = new_doc_path
            else:
                # æœ€åä¸€æ¬¡å°è¯•å¤±è´¥ï¼ŒæŠ›å‡ºé”™è¯¯å¹¶ç»™å‡ºæç¤º
                st.error(f"âŒ ä¿å­˜å¤±è´¥ï¼šæ–‡ä»¶å¯èƒ½æ­£åœ¨è¢«ä½¿ç”¨")
                st.warning(f"ğŸ’¡ è§£å†³æ–¹æ³•ï¼š\n1. å…³é—­å·²æ‰“å¼€çš„Wordæ–‡æ¡£\n2. é‡æ–°ç‚¹å‡»ç”ŸæˆæŒ‰é’®\n3. æˆ–ç­‰å¾…æ–‡ä»¶è‡ªåŠ¨ä¿å­˜ä¸ºå‰¯æœ¬")
                raise
        except Exception as e:
            st.error(f"âŒ ä¿å­˜å¤±è´¥ï¼š{str(e)}")
            raise
    
    return doc_path


# ============ Streamlitåº”ç”¨ ============
st.set_page_config(page_title="æœŸè´§æ—¥æŠ¥ç”Ÿæˆå™¨ï¼ˆAIèµ‹èƒ½ç‰ˆï¼‰", page_icon="ğŸ“Š", layout="wide")

# ============ ä¾§è¾¹æ ï¼šç³»ç»Ÿä¿¡æ¯ ============
st.sidebar.title("ğŸ“Š æœŸè´§æ—¥æŠ¥ç”Ÿæˆå™¨")
st.sidebar.markdown("**AIèµ‹èƒ½ç‰ˆ**")
st.sidebar.markdown("---")

# ç³»ç»ŸçŠ¶æ€
st.sidebar.subheader("âœ… ç³»ç»ŸçŠ¶æ€")
st.sidebar.success("ğŸ¤– DeepSeek AI - å·²å°±ç»ª")
st.sidebar.success("ğŸ” Serperæœç´¢ - å·²å°±ç»ª")
st.sidebar.success("ğŸ“ˆ æ•°æ®æ¥å£ - å·²å°±ç»ª")

st.sidebar.markdown("---")

# åŠŸèƒ½è¯´æ˜
st.sidebar.subheader("ğŸ¯ æ ¸å¿ƒåŠŸèƒ½")
st.sidebar.markdown("""
- ğŸ“Š è‡ªåŠ¨ç”ŸæˆKçº¿å›¾
- ğŸ¤– AIæ™ºèƒ½è¡Œæƒ…åˆ†æ
- ğŸ§  8ç»´åº¦ä¸“ä¸šè§‚ç‚¹
- ğŸ“° å¤šæºæ–°é—»èšåˆ
- ğŸ“„ ä¸€é”®ç”Ÿæˆæ—¥æŠ¥
""")

st.sidebar.markdown("---")
st.sidebar.info("ğŸ’¡ æ‰€æœ‰AIåŠŸèƒ½å·²å†…ç½®é…ç½®ï¼Œå¯ç›´æ¥ä½¿ç”¨")

# ============ ä¸»ç•Œé¢ ============
st.title("ğŸ“Š æœŸè´§æ—¥æŠ¥ç”Ÿæˆå™¨ï¼ˆAIèµ‹èƒ½ç‰ˆï¼‰")
st.write("**created by 7haoge (953534947@qq.com)**")

st.markdown("---")

# æ˜¾ç¤ºç³»ç»Ÿè¯´æ˜
with st.expander("ğŸ“– ç³»ç»Ÿä½¿ç”¨è¯´æ˜", expanded=False):
    st.markdown("""
    ### ğŸ“Š ç³»ç»Ÿç®€ä»‹
    
    **æœŸè´§æ—¥æŠ¥ç”Ÿæˆå™¨ï¼ˆAIèµ‹èƒ½ç‰ˆï¼‰** æ˜¯ä¸€æ¬¾ä¸“ä¸šçš„æœŸè´§æ—¥æŠ¥è‡ªåŠ¨ç”Ÿæˆå·¥å…·ï¼Œåˆ©ç”¨AIæŠ€æœ¯å¸®åŠ©æ‚¨å¿«é€Ÿç”Ÿæˆä¸“ä¸šçš„æœŸè´§å¸‚åœºæ—¥æŠ¥ã€‚
    
    ### âœ¨ æ ¸å¿ƒåŠŸèƒ½
    
    1. **ğŸ“ˆ Kçº¿å›¾è‡ªåŠ¨ç”Ÿæˆ**
       - è·å–æœŸè´§å“ç§çš„å®æ—¶è¡Œæƒ…æ•°æ®
       - è‡ªåŠ¨ç»˜åˆ¶ä¸“ä¸šKçº¿èµ°åŠ¿å›¾
       - åŒ…å«æ—¥ç›˜å’Œå¤œç›˜å®Œæ•´æ•°æ®
    
    2. **ğŸ¤– AIæ™ºèƒ½åˆ†æ**
       - AIè‡ªåŠ¨ç”Ÿæˆè¡Œæƒ…æè¿°
       - AIç»¼åˆåˆ†æç”ŸæˆæŠ•èµ„è§‚ç‚¹
       - åŸºäºDeepSeekå¤§æ¨¡å‹é©±åŠ¨
    
    3. **ğŸ“° å¤šæºæ–°é—»èšåˆ**
       - è‡ªåŠ¨æŠ“å–ä¸œæ–¹è´¢å¯Œã€é‡‘èç•Œç­‰è´¢ç»ç½‘ç«™æ–°é—»
       - æ™ºèƒ½ç­›é€‰ä¸å“ç§ç›¸å…³çš„èµ„è®¯
       - æŒ‰ç›¸å…³æ€§æ’åºï¼Œç²¾é€‰æœ€é‡è¦çš„10æ¡
    
    4. **ğŸ“„ ä¸“ä¸šæŠ¥å‘Šè¾“å‡º**
       - ç¬¦åˆæœºæ„æ ‡å‡†çš„Wordæ–‡æ¡£æ ¼å¼
       - åŒ…å«å®Œæ•´çš„æŠ¥å‘Šç»“æ„å’Œå…è´£å£°æ˜
       - ä¸€é”®ä¸‹è½½ï¼Œç›´æ¥å¯ç”¨
    
    ### ğŸš€ ä½¿ç”¨æµç¨‹
    
    **ç¬¬ä¸€æ­¥ï¼šè¾“å…¥åŸºæœ¬ä¿¡æ¯**
    - é€‰æ‹©æ—¥æœŸ
    - è¾“å…¥å“ç§åç§°ï¼ˆå¦‚ï¼šé“œã€èºçº¹é’¢ã€PTAç­‰ï¼‰
    - è¾“å…¥åˆçº¦ä»£ç ï¼ˆå¦‚ï¼šCU2501ã€RB2501ç­‰ï¼‰
    
    **ç¬¬äºŒæ­¥ï¼šç”ŸæˆKçº¿å›¾**
    - ç‚¹å‡»"ç”ŸæˆKçº¿å›¾"æŒ‰é’®
    - ç³»ç»Ÿè‡ªåŠ¨è·å–å¸‚åœºæ•°æ®å¹¶ç”Ÿæˆå›¾è¡¨
    - è‡ªåŠ¨è·å–ç›¸å…³æ–°é—»èµ„è®¯
    
    **ç¬¬ä¸‰æ­¥ï¼šç¼–è¾‘æŠ¥å‘Šå†…å®¹ï¼ˆAIæ™ºèƒ½ + äººå·¥å®¡æ ¸ï¼‰**
    
    - **ğŸ“ è¡Œæƒ…æè¿°**ï¼š
      - ç‚¹å‡»"AIç”Ÿæˆè¡Œæƒ…æè¿°"ï¼šåŸºäºçœŸå®è¡Œæƒ…æ•°æ®ï¼Œè‡ªåŠ¨ç”Ÿæˆä¸“ä¸šå¸‚åœºæè¿°
      - æˆ–æ‰‹åŠ¨ç¼–è¾‘ï¼Œçµæ´»è°ƒæ•´
    
    - **ğŸ’¡ ä¸»è¦è§‚ç‚¹ï¼ˆä¸“ä¸šç‰ˆï¼‰**ï¼šâ­ åŸºäº8å¤§ä¸“ä¸šç»´åº¦ç³»ç»Ÿåˆ†æ
      - ç‚¹å‡»"AIç”Ÿæˆä¸»è¦è§‚ç‚¹ï¼ˆä¸“ä¸šç‰ˆï¼‰"
      - AIå°†ç»¼åˆä»¥ä¸‹8ä¸ªç»´åº¦è¿›è¡Œä¸“ä¸šåˆ†æï¼š
        1. ğŸ“Š æŠ€æœ¯é¢åˆ†æï¼ˆMAã€MACDã€RSIã€å¸ƒæ—å¸¦ï¼‰
        2. ğŸ“¦ åŸºæœ¬é¢åˆ†æï¼ˆåº“å­˜ã€ä»“å•ã€ä¾›éœ€ã€åŸºå·®ï¼‰
        3. ğŸ’° èµ„é‡‘é¢åˆ†æï¼ˆæŒä»“å¸­ä½ã€ä¸»åŠ›åŠ¨å‘ï¼‰
        4. ğŸ”— äº§ä¸šé“¾åˆ†æï¼ˆä¸Šä¸‹æ¸¸ä»·æ ¼ã€åˆ©æ¶¦ï¼‰
        5. ğŸ“œ æ”¿ç­–é¢åˆ†æï¼ˆå›½å®¶æ”¿ç­–ã€è¡Œä¸šæ”¿ç­–ï¼‰
        6. ğŸŒ è¿›å‡ºå£åˆ†æï¼ˆæµ·å…³æ•°æ®ã€è´¸æ˜“æƒ…å†µï¼‰
        7. ğŸ“° å¸‚åœºæƒ…ç»ªï¼ˆæ–°é—»èˆ†æƒ…ã€æŠ•èµ„è€…æƒ…ç»ªï¼‰
        8. âš ï¸ é£é™©å› ç´ ï¼ˆä¸»è¦é£é™©ã€ä¸ç¡®å®šæ€§ï¼‰
      - ç”Ÿæˆåå¯æ‰‹åŠ¨ç¼–è¾‘å’Œè°ƒæ•´
    
    - **ğŸ“° æ–°é—»èµ„è®¯ï¼ˆNEW!ï¼‰**ï¼š
      - ç‚¹å‡»"AIç”Ÿæˆæ–°é—»èµ„è®¯"ï¼šAIè‡ªåŠ¨æ•´ç†æŒ‡å®šæ—¥æœŸæœç´¢åˆ°çš„æ‰€æœ‰æ–°é—»
      - åŒ…å«ï¼šå•†å“æ–°é—»ã€åº“å­˜æ•°æ®ã€åŸºå·®æ•°æ®ã€æŒä»“å˜åŒ–ç­‰å¤šç»´åº¦ä¿¡æ¯
      - åªå±•ç¤ºæ–°é—»ä¸»ä½“å†…å®¹ï¼Œè‡ªåŠ¨æ ‡æ³¨å¼•ç”¨[1][2][3]
      - è‡ªåŠ¨å»é‡å¹¶ç»Ÿä¸€æ ¼å¼
      - æ”¯æŒæ‰‹åŠ¨ç¼–è¾‘ã€åˆ å‡æˆ–è¡¥å……
    
    **ç¬¬å››æ­¥ï¼šç”Ÿæˆå®Œæ•´æ—¥æŠ¥**
    - ç‚¹å‡»"ç”Ÿæˆå®Œæ•´æ—¥æŠ¥"æŒ‰é’®
    - Wordæ–‡æ¡£è‡ªåŠ¨ä¿å­˜åˆ°æ¡Œé¢ã€ŒæœŸè´§æ—¥æŠ¥ã€æ–‡ä»¶å¤¹
    - ä¹Ÿå¯ä»¥ç›´æ¥ä¸‹è½½
    
    ### ğŸ’¡ ä½¿ç”¨æŠ€å·§
    
    - **åˆçº¦ä»£ç æ ¼å¼**ï¼šå“ç§ä»£ç ï¼ˆå¤§å†™ï¼‰+ å¹´ä»½ï¼ˆ2ä½ï¼‰+ æœˆä»½ï¼ˆ2ä½ï¼‰
      - ä¾‹å¦‚ï¼šCU2501ï¼ˆ2025å¹´1æœˆäº¤å‰²çš„é“œåˆçº¦ï¼‰
    - **AIç”Ÿæˆå†…å®¹**ï¼šå¯ä½œä¸ºåˆç¨¿å‚è€ƒï¼Œå»ºè®®æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´
    - **æ—¥æŠ¥ç”Ÿæˆæ—¶é—´**ï¼šå»ºè®®åœ¨æ¯æ—¥æ”¶ç›˜å15:30-16:00ç”Ÿæˆ
    
    ### âš ï¸ æ³¨æ„äº‹é¡¹
    
    - ç³»ç»Ÿéœ€è¦ç¨³å®šçš„ç½‘ç»œè¿æ¥è·å–æ•°æ®
    - åˆçº¦ä»£ç å¿…é¡»å‡†ç¡®ï¼Œå¦åˆ™æ— æ³•è·å–æ•°æ®
    - ç”Ÿæˆçš„æŠ¥å‘Šä»…ä¾›å‚è€ƒï¼Œä¸æ„æˆæŠ•èµ„å»ºè®®
    - æ—¥æŠ¥å°†ä¿å­˜åœ¨æ¡Œé¢çš„"æœŸè´§æ—¥æŠ¥"æ–‡ä»¶å¤¹ä¸­
    
    ### ğŸ“ æŠ€æœ¯æ”¯æŒ
    
    å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œæ¬¢è¿è”ç³»ï¼š
    - **ä½œè€…**ï¼š7haoge
    - **é‚®ç®±**ï¼š953534947@qq.com
    """)

# ä¸»ç•Œé¢
col1, col2 = st.columns([1, 1])

with col1:
    st.subheader("ğŸ“… åŸºæœ¬ä¿¡æ¯")
    custom_date = st.date_input("è¯·é€‰æ‹©æ—¥æœŸ", datetime.now())
    
    # å“ç§åè¾“å…¥
    commodity_name = st.text_input(
        "è¯·è¾“å…¥å“ç§åç§°",
        placeholder="ä¾‹å¦‚ï¼šé“œã€èºçº¹é’¢ã€PTAã€è±†ç²•ã€ç™½é“¶ç­‰",
        help="ç›´æ¥è¾“å…¥å“ç§ä¸­æ–‡åç§°"
    )
    
    # æ˜¾ç¤ºåˆçº¦ä»£ç ç¤ºä¾‹
    example_contract = ""
    if commodity_name and commodity_name in COMMODITY_EXAMPLES:
        example_contract = COMMODITY_EXAMPLES[commodity_name]
    
    # åˆçº¦ä»£ç è¾“å…¥
    full_contract = st.text_input(
        "è¯·è¾“å…¥å®Œæ•´å“ç§åˆçº¦",
        placeholder=f"ä¾‹å¦‚ï¼š{example_contract if example_contract else 'CU2501ã€RB2501ã€AG2512'}",
        help="æ ¼å¼ï¼šå“ç§ä»£ç +å¹´æœˆï¼Œå¦‚CU2501è¡¨ç¤º2025å¹´1æœˆäº¤å‰²çš„é“œåˆçº¦"
    )

with col2:
    pass  # æ•°æ®é¢„è§ˆéƒ¨åˆ†å·²åˆ é™¤

st.markdown("---")

# åˆå§‹åŒ–session state
if 'market_data_dict' not in st.session_state:
    st.session_state.market_data_dict = {}
if 'news_list' not in st.session_state:
    st.session_state.news_list = []
if 'day_description' not in st.session_state:
    st.session_state.day_description = ""
if 'night_description' not in st.session_state:
    st.session_state.night_description = ""
if 'ai_generated_description' not in st.session_state:
    st.session_state.ai_generated_description = ""
if 'ai_generated_view' not in st.session_state:
    st.session_state.ai_generated_view = ""
if 'ai_generated_news' not in st.session_state:
    st.session_state.ai_generated_news = ""
if 'professional_data' not in st.session_state:
    st.session_state.professional_data = {}
if 'commodity_name' not in st.session_state:
    st.session_state.commodity_name = ""
if 'full_contract' not in st.session_state:
    st.session_state.full_contract = ""
if 'custom_date' not in st.session_state:
    st.session_state.custom_date = datetime.now()
if 'temp_ai_desc' not in st.session_state:
    st.session_state.temp_ai_desc = ""
if 'temp_ai_view' not in st.session_state:
    st.session_state.temp_ai_view = ""
if 'temp_ai_news' not in st.session_state:
    st.session_state.temp_ai_news = ""

# Kçº¿å›¾ç”Ÿæˆ
if st.button("ğŸ¨ ç”ŸæˆKçº¿å›¾", type="primary"):
    if not full_contract or not commodity_name:
        st.error("âŒ è¯·å…ˆè¾“å…¥å“ç§åç§°å’Œå®Œæ•´åˆçº¦ä»£ç ")
    else:
        # ä¿å­˜ç”¨æˆ·è¾“å…¥åˆ°session state
        st.session_state.commodity_name = commodity_name
        st.session_state.full_contract = full_contract
        st.session_state.custom_date = custom_date
        
        with st.spinner("æ­£åœ¨ç”ŸæˆKçº¿å›¾..."):
            custom_date_str = custom_date.strftime('%Y-%m-%d')
            
            # è·å–å¸‚åœºæ•°æ®ï¼ˆè¿”å›5ä¸ªå€¼ï¼‰
            day_description, night_description, market_data, market_data_dict, actual_market_date = get_market_trend_data(full_contract, custom_date)
            
            # å¦‚æœæ‰¾ä¸åˆ°äº¤æ˜“æ—¥ï¼Œåœæ­¢
            if actual_market_date is None:
                st.error("æ— æ³•æ‰¾åˆ°æœ‰æ•ˆçš„äº¤æ˜“æ—¥æ•°æ®ï¼Œè¯·æ£€æŸ¥åˆçº¦ä»£ç æˆ–é€‰æ‹©å…¶ä»–æ—¥æœŸ")
                st.stop()
            
            # ä¿å­˜åˆ°session state
            st.session_state.market_data_dict = market_data_dict
            st.session_state.actual_market_date = actual_market_date  # ä¿å­˜å®é™…äº¤æ˜“æ—¥
            st.session_state.market_data_df = market_data  # ä¿å­˜DataFrameç”¨äºæŠ€æœ¯æŒ‡æ ‡è®¡ç®—
            st.session_state.day_description = day_description
            st.session_state.night_description = night_description
            
            # ç”Ÿæˆèµ„è®¯æ—¥æœŸåˆ—è¡¨ï¼šä»äº¤æ˜“æ—¥åˆ°æŠ¥å‘Šæ—¥æœŸçš„æ‰€æœ‰æ—¥æœŸ
            news_date_list = []
            current_date = actual_market_date
            while current_date <= custom_date:
                news_date_list.append(current_date.strftime('%Y-%m-%d'))
                current_date += timedelta(days=1)
            
            st.info(f"ğŸ“° èµ„è®¯æœç´¢èŒƒå›´ï¼š{actual_market_date.strftime('%Y-%m-%d')} è‡³ {custom_date_str}ï¼ˆå…±{len(news_date_list)}å¤©ï¼‰")
            
            # è·å–æ–°é—»ï¼ˆä¼ å…¥æ—¥æœŸåˆ—è¡¨ï¼‰
            news_description, news_list = get_news_data_enhanced(
                commodity_name, 
                SERPER_API_KEY,
                target_dates=news_date_list  # ä¼ å…¥æ—¥æœŸåˆ—è¡¨
            )
            st.session_state.news_list = news_list
            
            if not market_data.empty:
                k_line_chart_path = create_k_line_chart(market_data, full_contract, ".")
                
                if k_line_chart_path:
                    col_img1, col_img2 = st.columns([2, 1])
                    with col_img1:
                        st.image(k_line_chart_path, caption=f"{commodity_name}æ˜¨æ—¥Kçº¿å›¾", use_container_width=True)
                    with col_img2:
                        st.write("**ğŸ“Š æ˜¨æ—¥èµ°åŠ¿ï¼š**")
                        st.write(day_description)
                        st.write("")
                        st.write("**ğŸŒ™ å¤œç›˜èµ°åŠ¿ï¼š**")
                        st.write(night_description)
                else:
                    st.error("âŒ æ— æ³•ç”ŸæˆKçº¿å›¾")
            else:
                st.error("âŒ æ— æ³•è·å–å¸‚åœºæ•°æ®ï¼Œè¯·æ£€æŸ¥åˆçº¦ä»£ç æ˜¯å¦æ­£ç¡®")

st.markdown("---")

# å†…å®¹ç¼–è¾‘åŒºåŸŸ
st.subheader("âœï¸ ç¼–è¾‘æŠ¥å‘Šå†…å®¹")

# è¡Œæƒ…æè¿°åŒºåŸŸ
st.markdown("### ğŸ“ è¡Œæƒ…æè¿°")
col_desc1, col_desc2 = st.columns([3, 1])

with col_desc1:
    # ç¡®å®šæ˜¾ç¤ºçš„å†…å®¹ï¼šä¼˜å…ˆæ˜¾ç¤ºAIç”Ÿæˆçš„ï¼Œå¦åˆ™æ˜¾ç¤ºè‡ªåŠ¨ç”Ÿæˆçš„
    default_description = st.session_state.get('ai_generated_description', '')
    if not default_description:
        default_description = st.session_state.get('day_description', '') + '\n\n' + st.session_state.get('night_description', '')
    
    user_description = st.text_area(
        "è¯·è¾“å…¥è¡Œæƒ…æè¿°ï¼ˆå¯é‡‡ç”¨è‡ªåŠ¨ç”Ÿæˆçš„æ–‡æ¡ˆæˆ–è‡ªè¡Œç¼–è¾‘ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨AIç”Ÿæˆï¼‰",
        value=default_description,
        height=200,
        key="user_description"
    )

with col_desc2:
    st.write("")
    st.write("")
    
    if st.button("ğŸ¤– AIç”Ÿæˆè¡Œæƒ…æè¿°", use_container_width=True, key="btn_gen_desc"):
        if not st.session_state.get('market_data_dict'):
            st.warning("âš ï¸ è¯·å…ˆç”ŸæˆKçº¿å›¾ä»¥è·å–å¸‚åœºæ•°æ®")
        elif not st.session_state.get('commodity_name'):
            st.warning("âš ï¸ è¯·å…ˆè¾“å…¥å“ç§åç§°å¹¶ç”ŸæˆKçº¿å›¾")
        else:
            try:
                with st.spinner("ğŸ¤– AIæ­£åœ¨ç”Ÿæˆ..."):
                    # è·å–æŠ€æœ¯æŒ‡æ ‡
                    technical_indicators = {}
                    if 'market_data_df' in st.session_state and not st.session_state.market_data_df.empty:
                        technical_indicators = calculate_technical_indicators(st.session_state.market_data_df)
                    
                    ai_desc = ai_generate_market_description(
                        st.session_state.market_data_dict,
                        st.session_state.commodity_name,
                        st.session_state.custom_date.strftime('%Y-%m-%d'),
                        technical_indicators=technical_indicators,
                        market_date=st.session_state.get('actual_market_date')  # ä¼ é€’å®é™…äº¤æ˜“æ—¥
                    )
                
                if ai_desc and len(ai_desc) > 50:
                    # ä¿å­˜åˆ°session_state
                    st.session_state.temp_ai_desc = ai_desc
                    st.success("âœ… ç”ŸæˆæˆåŠŸï¼å†…å®¹æ˜¾ç¤ºåœ¨ä¸‹æ–¹")
                else:
                    st.error(f"âŒ ç”Ÿæˆå¤±è´¥: {ai_desc}")
            except Exception as e:
                st.error(f"âŒ å¼‚å¸¸: {str(e)}")
    
    # æ˜¾ç¤ºå·²ç”Ÿæˆçš„å†…å®¹ï¼ˆå¦‚æœæœ‰ï¼‰
    if st.session_state.get('temp_ai_desc'):
        st.write("ğŸ“ ç”Ÿæˆçš„è¡Œæƒ…æè¿°ï¼š")
        st.text_area("AIç”Ÿæˆçš„è¡Œæƒ…æè¿°", value=st.session_state.temp_ai_desc, height=200, key="display_ai_desc", label_visibility="collapsed")
        st.caption("ğŸ’¡ è¯·å¤åˆ¶ä¸Šé¢çš„å†…å®¹åˆ°ä¸Šæ–¹è¾“å…¥æ¡†")

# ä¸»è¦è§‚ç‚¹åŒºåŸŸ
st.markdown("### ğŸ’¡ ä¸»è¦è§‚ç‚¹")
col_view1, col_view2 = st.columns([3, 1])

with col_view1:
    # ç¡®å®šæ˜¾ç¤ºçš„å†…å®¹ï¼šä¼˜å…ˆæ˜¾ç¤ºAIç”Ÿæˆçš„
    default_view = st.session_state.get('ai_generated_view', '')
    
    main_view = st.text_area(
        "è¯·è¾“å…¥ä¸»è¦è§‚ç‚¹ï¼ˆå¯è‡ªè¡Œç¼–è¾‘æˆ–AIç”Ÿæˆï¼‰",
        value=default_view,
        height=200,
        key="main_view",
        placeholder="è¾“å…¥æ‚¨å¯¹å¸‚åœºçš„ä¸»è¦åˆ¤æ–­å’ŒæŠ•èµ„å»ºè®®..."
    )

with col_view2:
    st.write("")
    st.write("")
    if st.button("ğŸ§  AIç”Ÿæˆä¸»è¦è§‚ç‚¹ï¼ˆä¸“ä¸šç‰ˆï¼‰", use_container_width=True, key="btn_gen_view"):
        if not st.session_state.get('market_data_dict'):
            st.warning("âš ï¸ è¯·å…ˆç”ŸæˆKçº¿å›¾ä»¥è·å–å¸‚åœºæ•°æ®")
        elif not st.session_state.get('commodity_name'):
            st.warning("âš ï¸ è¯·å…ˆè¾“å…¥å“ç§åç§°å¹¶ç”ŸæˆKçº¿å›¾")
        else:
            try:
                # ç¬¬1æ­¥ï¼šè®¡ç®—æŠ€æœ¯æŒ‡æ ‡
                with st.spinner("ğŸ“Š æ­£åœ¨è®¡ç®—æŠ€æœ¯æŒ‡æ ‡..."):
                    market_data_df = st.session_state.get('market_data_df', pd.DataFrame())
                    technical_indicators = calculate_technical_indicators(market_data_df)
                
                # ç¬¬2æ­¥ï¼šæœç´¢8å¤§ç»´åº¦ä¸“ä¸šæ•°æ®
                with st.spinner("ğŸ” æ­£åœ¨æœç´¢8å¤§ç»´åº¦ä¸“ä¸šæ•°æ®..."):
                    searcher = EnhancedNewsSearcher()
                    professional_data = searcher.search_professional_data(
                        st.session_state.commodity_name,
                        SERPER_API_KEY,
                        st.session_state.custom_date.strftime('%Y-%m-%d')
                    )
                    st.session_state.professional_data = professional_data if professional_data else {}
                
                # ç¬¬3æ­¥ï¼šAIç»¼åˆåˆ†æç”Ÿæˆè§‚ç‚¹
                with st.spinner("ğŸ¤– AIæ­£åœ¨è¿›è¡Œ8å¤§ç»´åº¦ä¸“ä¸šåˆ†æ..."):
                    ai_view = ai_generate_main_view(
                        st.session_state.commodity_name,
                        st.session_state.custom_date.strftime('%Y-%m-%d'),
                        st.session_state.market_data_dict,
                        st.session_state.news_list,
                        professional_data,
                        technical_indicators,
                        market_date=st.session_state.get('actual_market_date')  # ä¼ é€’å®é™…äº¤æ˜“æ—¥
                    )
                
                if ai_view and len(ai_view) > 50:
                    # ä¿å­˜åˆ°session_state
                    st.session_state.temp_ai_view = ai_view
                    st.success("âœ… ä¸»è¦è§‚ç‚¹ç”Ÿæˆå®Œæˆï¼å†…å®¹æ˜¾ç¤ºåœ¨ä¸‹æ–¹")
                else:
                    st.error(f"âŒ ç”Ÿæˆå¤±è´¥: {ai_view}")
            except Exception as e:
                st.error(f"âŒ å¼‚å¸¸: {str(e)}")
    
    # æ˜¾ç¤ºå·²ç”Ÿæˆçš„å†…å®¹ï¼ˆå¦‚æœæœ‰ï¼‰
    if st.session_state.get('temp_ai_view'):
        st.write("ğŸ“ ç”Ÿæˆçš„ä¸»è¦è§‚ç‚¹ï¼š")
        st.text_area("AIç”Ÿæˆçš„ä¸»è¦è§‚ç‚¹", value=st.session_state.temp_ai_view, height=250, key="display_ai_view", label_visibility="collapsed")
        st.caption("ğŸ’¡ è¯·å¤åˆ¶ä¸Šé¢çš„å†…å®¹åˆ°ä¸Šæ–¹è¾“å…¥æ¡†")

# æ–°é—»èµ„è®¯åŒºåŸŸ
st.markdown("### ğŸ“° æ–°é—»èµ„è®¯")
col_news1, col_news2 = st.columns([3, 1])

with col_news1:
    # ç¡®å®šæ˜¾ç¤ºçš„å†…å®¹ï¼šä¼˜å…ˆæ˜¾ç¤ºAIç”Ÿæˆçš„
    default_news = st.session_state.get('ai_generated_news', '')
    
    news_content = st.text_area(
        "è¯·è¾“å…¥æ–°é—»èµ„è®¯ï¼ˆå¯è‡ªè¡Œç¼–è¾‘æˆ–AIç”Ÿæˆï¼‰",
        value=default_news,
        height=300,
        key="news_content",
        placeholder="""è¾“å…¥æˆ–ç”Ÿæˆæ–°é—»èµ„è®¯ï¼Œæ ¼å¼ç¤ºä¾‹ï¼š

1. ä»Šæ—¥ç”ŸçŒªæœŸè´§ä¸»åŠ›åˆçº¦æ”¶äº11325å…ƒï¼Œæ—¥å†…ä¸‹è·Œ2.12%ï¼Œåˆ›è¿‘æœŸæ–°ä½[1]

2. å…¨å›½ç”ŸçŒªå‡ä»·è·Œè‡³11.19å…ƒ/å…¬æ–¤ï¼Œç¯æ¯”ä¸‹è·Œ0.26å…ƒï¼Œæœˆç¯æ¯”ä¸‹è·Œ16.04%[2]

3. äº¤æ˜“æ‰€ä»“å•æ•°æ®æ˜¾ç¤ºï¼Œç”ŸçŒªä»“å•ç¯æ¯”å¢åŠ 3000å¨è‡³15000å¨ï¼Œä¾›åº”å‹åŠ›æŒç»­[3]

...

ğŸ’¡ æç¤ºï¼šå¯æ‰‹åŠ¨è¾“å…¥ï¼Œä¹Ÿå¯ç‚¹å‡»å³ä¾§"AIç”Ÿæˆ"æŒ‰é’®è‡ªåŠ¨æ•´ç†"""
    )

with col_news2:
    st.write("")
    st.write("")
    if st.button("ğŸ“° AIç”Ÿæˆæ–°é—»èµ„è®¯", use_container_width=True, key="btn_gen_news"):
        if not st.session_state.get('news_list'):
            st.warning("âš ï¸ è¯·å…ˆç”ŸæˆKçº¿å›¾ä»¥è·å–æ–°é—»æ•°æ®")
        elif not st.session_state.get('commodity_name'):
            st.warning("âš ï¸ è¯·å…ˆè¾“å…¥å“ç§åç§°å¹¶ç”ŸæˆKçº¿å›¾")
        else:
            try:
                # å¦‚æœprofessional_dataä¸å­˜åœ¨ï¼Œå…ˆæœç´¢ä¸€æ¬¡
                professional_data = st.session_state.get('professional_data')
                if not professional_data:
                    with st.spinner("ğŸ” æ­£åœ¨æœç´¢ä¸“ä¸šæ•°æ®..."):
                        searcher = EnhancedNewsSearcher()
                        professional_data = searcher.search_professional_data(
                            st.session_state.commodity_name,
                            SERPER_API_KEY,
                            st.session_state.custom_date.strftime('%Y-%m-%d')
                        )
                        st.session_state.professional_data = professional_data if professional_data else {}
                
                with st.spinner("ğŸ¤– AIæ­£åœ¨æ•´ç†æ–°é—»èµ„è®¯..."):
                    ai_news = ai_generate_news_summary(
                        st.session_state.commodity_name,
                        st.session_state.custom_date.strftime('%Y-%m-%d'),
                        st.session_state.news_list,
                        professional_data or {}
                    )
                
                if ai_news and len(ai_news) > 50:
                    # ä¿å­˜åˆ°session_state
                    st.session_state.temp_ai_news = ai_news
                    st.success("âœ… æ–°é—»èµ„è®¯ç”Ÿæˆå®Œæˆï¼å†…å®¹æ˜¾ç¤ºåœ¨ä¸‹æ–¹")
                else:
                    st.error(f"âŒ ç”Ÿæˆå¤±è´¥: {ai_news}")
            except Exception as e:
                st.error(f"âŒ å¼‚å¸¸: {str(e)}")
    
    # æ˜¾ç¤ºå·²ç”Ÿæˆçš„å†…å®¹ï¼ˆå¦‚æœæœ‰ï¼‰
    if st.session_state.get('temp_ai_news'):
        st.write("ğŸ“ ç”Ÿæˆçš„æ–°é—»èµ„è®¯ï¼š")
        st.text_area("AIç”Ÿæˆçš„æ–°é—»èµ„è®¯", value=st.session_state.temp_ai_news, height=300, key="display_ai_news", label_visibility="collapsed")
        st.caption("ğŸ’¡ è¯·å¤åˆ¶ä¸Šé¢çš„å†…å®¹åˆ°ä¸Šæ–¹è¾“å…¥æ¡†")
    else:
        st.write("")
        st.info("ğŸ’¡ AIä¼šæ•´ç†æ‰€æœ‰æœç´¢åˆ°çš„æ–°é—»")

st.markdown("---")

# ç”Ÿæˆæ—¥æŠ¥æŒ‰é’®
if st.button("ğŸ“„ ç”Ÿæˆå®Œæ•´æ—¥æŠ¥", type="primary"):
    if not full_contract or not commodity_name:
        st.error("âŒ è¯·å…ˆè¾“å…¥å“ç§åç§°å’Œåˆçº¦ä»£ç ")
    elif not user_description or not main_view:
        st.error("âŒ è¯·å¡«å†™è¡Œæƒ…æè¿°å’Œä¸»è¦è§‚ç‚¹")
    elif not news_content:
        st.warning("âš ï¸ å»ºè®®å¡«å†™æ–°é—»èµ„è®¯å†…å®¹ï¼Œæˆ–ç‚¹å‡»AIç”Ÿæˆ")
        st.info("ğŸ’¡ æ‚¨ä¹Ÿå¯ä»¥ç•™ç©ºæ–°é—»èµ„è®¯ï¼Œç»§ç»­ç”Ÿæˆæ—¥æŠ¥")
    
    # æ— è®ºæ–°é—»èµ„è®¯æ˜¯å¦å¡«å†™ï¼Œéƒ½å…è®¸ç”Ÿæˆï¼ˆå…¼å®¹æ€§ï¼‰
    if (full_contract and commodity_name and user_description and main_view):
        try:
            with st.spinner("æ­£åœ¨ç”Ÿæˆæ—¥æŠ¥ï¼Œè¯·ç¨å€™..."):
                custom_date_str = custom_date.strftime('%Y-%m-%d')
                doc_path = create_report_professional(
                    custom_date_str,
                    full_contract,
                    commodity_name,
                    user_description,
                    main_view,
                    user_news_content=news_content,  # ä¼ å…¥ç”¨æˆ·ç¼–è¾‘çš„æ–°é—»å†…å®¹
                    serper_key=SERPER_API_KEY
                )
                
                if doc_path:
                    st.success("âœ… æ—¥æŠ¥ç”ŸæˆæˆåŠŸï¼")
                    
                    # æ˜¾ç¤ºä¿å­˜è·¯å¾„å’Œæ–‡ä»¶å
                    st.info(f"ğŸ“ æŠ¥å‘Šå·²ä¿å­˜è‡³ï¼š`{os.path.dirname(doc_path)}`")
                    st.info(f"ğŸ“„ æ–‡ä»¶åï¼š`{os.path.basename(doc_path)}`")
                    
                    # æç¤ºå¦‚ä½•æŸ¥çœ‹
                    st.markdown("ğŸ’¡ **æŸ¥çœ‹æŠ¥å‘Šï¼š** å¯åœ¨æ¡Œé¢ã€ŒæœŸè´§æ—¥æŠ¥ã€æ–‡ä»¶å¤¹ä¸­æ‰¾åˆ°ç”Ÿæˆçš„Wordæ–‡æ¡£")
                    
                    with open(doc_path, "rb") as f:
                        st.download_button(
                            label="ğŸ“¥ ä¸‹è½½æ—¥æŠ¥",
                            data=f,
                            file_name=os.path.basename(doc_path),
                            mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
                            use_container_width=True
                        )
                    
                    st.balloons()
                else:
                    st.error("âŒ æ—¥æŠ¥ç”Ÿæˆå¤±è´¥ï¼Œè¯·é‡è¯•")
        except PermissionError:
            st.error("âŒ æ–‡ä»¶ä¿å­˜å¤±è´¥ï¼šæƒé™è¢«æ‹’ç»")
            st.warning("""
            **ğŸ’¡ å¸¸è§åŸå› å’Œè§£å†³æ–¹æ³•ï¼š**
            
            1. **Wordæ–‡æ¡£æ­£åœ¨æ‰“å¼€** 
               - è¯·å…³é—­æ¡Œé¢ã€ŒæœŸè´§æ—¥æŠ¥ã€æ–‡ä»¶å¤¹ä¸­å·²æ‰“å¼€çš„åŒåWordæ–‡æ¡£
               - ç„¶åé‡æ–°ç‚¹å‡»"ç”Ÿæˆå®Œæ•´æ—¥æŠ¥"æŒ‰é’®
            
            2. **æ–‡ä»¶è¢«å ç”¨**
               - ç³»ç»Ÿå·²è‡ªåŠ¨å°è¯•ç”Ÿæˆå¸¦åºå·çš„å‰¯æœ¬æ–‡ä»¶
               - è¯·æŸ¥çœ‹æ–‡ä»¶å¤¹ä¸­æ˜¯å¦æœ‰ã€ŒæœŸè´§æ—¥æŠ¥_æ—¥æœŸ_å‰¯æœ¬1.docxã€ç­‰æ–‡ä»¶
            
            3. **æƒé™ä¸è¶³**
               - ç¡®ä¿æ‚¨æœ‰æ¡Œé¢æ–‡ä»¶å¤¹çš„å†™å…¥æƒé™
               - å°è¯•ä»¥ç®¡ç†å‘˜èº«ä»½è¿è¡Œç¨‹åº
            """)
        except Exception as e:
            st.error(f"âŒ ç”Ÿæˆå¤±è´¥ï¼š{str(e)}")
            st.warning("""
            **ğŸ’¡ è¯·æ£€æŸ¥ï¼š**
            - ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸
            - æ‰€æœ‰å¿…å¡«ä¿¡æ¯æ˜¯å¦å®Œæ•´
            - æ˜¯å¦æœ‰è¶³å¤Ÿçš„ç£ç›˜ç©ºé—´
            - APIå¯†é’¥æ˜¯å¦æ­£ç¡®é…ç½®
            """)

# é¡µè„š
st.markdown("---")
st.markdown("""
<div style='text-align: center; color: gray; padding: 20px;'>
    <p>ğŸ“Š æœŸè´§æ—¥æŠ¥ç”Ÿæˆå™¨ï¼ˆAIèµ‹èƒ½ç‰ˆï¼‰</p>
    <p>ä¸“ä¸š | æ™ºèƒ½ | æ•ˆç‡</p>
    <p>Powered by DeepSeek</p>
    <p>Created by 7haogeï¼ˆ953534947@qq.comï¼‰</p>
</div>
""", unsafe_allow_html=True)

